import os
import sys
import csv
import json
import copy
import random
import torch
import numpy as np
import matplotlib.pyplot as plt
import swanlab
from itertools import product
from datetime import datetime
from torch.cuda.amp import autocast

# ====================== å…¨å±€é…ç½®ï¼šè§£å†³Linuxä¸­æ–‡æ˜¾ç¤º+å›ºå®šéšæœºç§å­ï¼ˆé€šç”¨é…ç½®ï¼Œæ— éœ€ä¿®æ”¹ï¼‰ ======================
plt.switch_backend('Agg')
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # çº¯è‹±æ–‡é¿å­—ä½“é—®é¢˜
plt.rcParams['axes.unicode_minus'] = False
# å›ºå®šéšæœºç§å­ä¿è¯å¯å¤ç°ï¼ˆè‹¥éœ€ä¿®æ”¹ï¼Œåœ¨YMLä¸­è®¾ç½®seedï¼Œè„šæœ¬ä¼šè¯»å–ï¼‰
BASE_SEED = 42
torch.manual_seed(BASE_SEED)
torch.cuda.manual_seed(BASE_SEED)
np.random.seed(BASE_SEED)
random.seed(BASE_SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ====================== âœ… ä»…éœ€ç”¨æˆ·ä¿®æ”¹è¿™5é¡¹ âœ… å…¶ä½™å…¨ç”±YMLæ§åˆ¶ ======================
CFG_PATH = "config/ut-zappos.yml"  # ä½ çš„ä¸»YMLé…ç½®æ–‡ä»¶è·¯å¾„
SAVE_DIR = "tune_improve_on2"    # è°ƒå‚ç»“æœä¿å­˜ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
SWANLAB_PROJECT = "Tune-Improve2"  # SwanLabé¡¹ç›®å
# è°ƒå‚å‚æ•°èŒƒå›´ï¼šæŒ‰åœºæ™¯é¢„ç•™ï¼Œè„šæœ¬ä¼šæ ¹æ®YMLä¸­çš„use_robust_cacheè‡ªåŠ¨åŒ¹é…
TUNE_PARAMS_SCOPE = {
    # åœºæ™¯1ï¼šYMLä¸­use_robust_cache=Falseï¼ˆä»…æ”¹è¿›ä¸€ï¼‰â†’ è°ƒè¿™ä¸¤ä¸ª
    "lambda_orth": [2,3,4,5,6,7,8,9,10],  # æ­£äº¤æŸå¤±æƒé‡
    "hier_theta": [4.5,5,5.5,6,6.5,7],               # è‡ªé€‚åº”æ›´æ–°æ¸©åº¦ç³»æ•°
    # åœºæ™¯2ï¼šYMLä¸­use_robust_cache=Trueï¼ˆæ”¹è¿›ä¸€+äºŒï¼‰â†’ è°ƒè¿™ä¸¤ä¸ª
    "sim_threshold": [0.05,0.7,0.10,0.12,0.15,0.17,0.20],           # ç¼“å­˜å…¥é˜Ÿç›¸ä¼¼åº¦é˜ˆå€¼
    "correction_interval": [10 ,15, 20,25,30,35,40]          # ç¼“å­˜å‘¨æœŸæ€§ä¿®æ­£æ­¥é•¿
}
# è¦è®°å½•çš„æ ¸å¿ƒæŒ‡æ ‡ï¼ˆå’Œswan_test2.pyè¾“å‡ºå®Œå…¨ä¸€è‡´ï¼Œæ— éœ€ä¿®æ”¹ï¼‰
CORE_METRICS = ["AUC", "best_hm", "attr_acc", "best_seen", "best_unseen", "obj_acc", "biasterm"]

# ====================== å…¨å±€å˜é‡ï¼šåŠ è½½YMLåè‡ªåŠ¨åˆå§‹åŒ–ï¼ˆç”¨æˆ·æ— éœ€ç®¡ï¼‰ ======================
DIR_PATH = os.path.dirname(os.path.abspath(__file__))
original_cfg = None  # åŸå§‹YMLé…ç½®
use_robust_cache = False  # ä»YMLè¯»å–åèµ‹å€¼
TUNE_PARAMS = []  # è‡ªåŠ¨åŒ¹é…çš„å¾…è°ƒå‚æ•°ï¼Œå¦‚["lambda_orth", "hier_theta"]
TUNE_PARAM1, TUNE_PARAM2 = "", ""  # å¾…è°ƒå‚æ•°1/2
TUNE_VALS1, TUNE_VALS2 = [], []    # å¾…è°ƒå‚æ•°1/2çš„èŒƒå›´
TOTAL_EXP_NUM = 0  # æ€»å®éªŒç»„æ•°ï¼Œè‡ªåŠ¨è®¡ç®—

# ====================== å®Œå…¨ä¿ç•™åŸæœ‰æ­£ç¡®é€»è¾‘ï¼šEvaluatorç±»ï¼ˆæŒ‡æ ‡è®¡ç®—æ ¸å¿ƒï¼Œä¸€è¡Œæœªæ”¹ï¼‰ ======================
class Evaluator:
    def __init__(self, dset, model, device):
        self.dset = dset
        self.device = device

        pairs = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in dset.pairs]
        self.train_pairs = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in dset.train_pairs]
        self.pairs = torch.LongTensor(pairs)

        if dset.phase == 'train':
            test_pair_set = set(dset.train_pairs)
            test_pair_gt = set(dset.train_pairs)
        elif dset.phase == 'val':
            test_pair_set = set(dset.val_pairs + dset.train_pairs)
            test_pair_gt = set(dset.val_pairs)
        else:
            test_pair_set = set(dset.test_pairs + dset.train_pairs)
            test_pair_gt = set(dset.test_pairs)

        self.test_pair_dict = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in test_pair_gt]
        self.test_pair_dict = dict.fromkeys(self.test_pair_dict, 0)

        for attr, obj in test_pair_gt:
            pair_val = dset.pair2idx[(attr, obj)]
            key = (dset.attr2idx[attr], dset.obj2idx[obj])
            self.test_pair_dict[key] = [pair_val, 0, 0]

        if dset.open_world:
            masks = [1 for _ in dset.pairs]
        else:
            masks = [1 if pair in test_pair_set else 0 for pair in dset.pairs]

        self.closed_mask = torch.BoolTensor(masks)
        seen_pair_set = set(dset.train_pairs)
        mask = [1 if pair in seen_pair_set else 0 for pair in dset.pairs]
        self.seen_mask = torch.BoolTensor(mask)

        oracle_obj_mask = []
        for _obj in dset.objs:
            mask = [1 if _obj == obj else 0 for attr, obj in dset.pairs]
            oracle_obj_mask.append(torch.BoolTensor(mask))
        self.oracle_obj_mask = torch.stack(oracle_obj_mask, 0)

        self.score_model = self.score_manifold_model

    def generate_predictions(self, scores, obj_truth, bias=0.0, topk=1):
        def get_pred_from_scores(_scores, topk):
            _, pair_pred = _scores.topk(topk, dim=1)
            pair_pred = pair_pred.contiguous().view(-1)
            attr_pred, obj_pred = self.pairs[pair_pred][:, 0].view(-1, topk), self.pairs[pair_pred][:, 1].view(-1, topk)
            return (attr_pred, obj_pred)

        results = {}
        orig_scores = scores.clone()
        mask = self.seen_mask.repeat(scores.shape[0], 1)
        scores[~mask] += bias

        results.update({"open": get_pred_from_scores(scores, topk)})
        results.update({"unbiased_open": get_pred_from_scores(orig_scores, topk)})
        mask = self.closed_mask.repeat(scores.shape[0], 1)
        closed_scores = scores.clone()
        closed_scores[~mask] = -1e10
        closed_orig_scores = orig_scores.clone()
        closed_orig_scores[~mask] = -1e10
        results.update({"closed": get_pred_from_scores(closed_scores, topk)})
        results.update({"unbiased_closed": get_pred_from_scores(closed_orig_scores, topk)})

        return results

    def score_clf_model(self, scores, obj_truth, topk=1):
        attr_pred, obj_pred = scores
        attr_pred, obj_pred, obj_truth = attr_pred.to('cpu'), obj_pred.to('cpu'), obj_truth.to('cpu')
        attr_subset = attr_pred.index_select(1, self.pairs[:, 0])
        obj_subset = obj_pred.index_select(1, self.pairs[:, 1])
        scores = (attr_subset * obj_subset)
        results = self.generate_predictions(scores, obj_truth)
        results['biased_scores'] = scores
        return results

    def score_manifold_model(self, scores, obj_truth, bias=0.0, topk=1):
        scores = {k: v.to('cpu') for k, v in scores.items()}
        obj_truth = obj_truth.to(self.device)
        scores = torch.stack([scores[(attr, obj)] for attr, obj in self.dset.pairs], 1)
        orig_scores = scores.clone()
        results = self.generate_predictions(scores, obj_truth, bias, topk)
        results['scores'] = orig_scores
        return results

    def score_fast_model(self, scores, obj_truth, bias=0.0, topk=1):
        results = {}
        mask = self.seen_mask.repeat(scores.shape[0], 1)
        scores[~mask] += bias
        mask = self.closed_mask.repeat(scores.shape[0], 1)
        closed_scores = scores.clone()
        closed_scores[~mask] = -1e10
        _, pair_pred = closed_scores.topk(topk, dim=1)
        pair_pred = pair_pred.contiguous().view(-1)
        attr_pred, obj_pred = self.pairs[pair_pred][:, 0].view(-1, topk), self.pairs[pair_pred][:, 1].view(-1, topk)
        results.update({'closed': (attr_pred, obj_pred)})
        return results

    def evaluate_predictions(self, predictions, attr_truth, obj_truth, pair_truth, allpred, topk=1):
        from scipy.stats import hmean
        attr_truth, obj_truth, pair_truth = attr_truth.to("cpu"), obj_truth.to("cpu"), pair_truth.to("cpu")
        pairs = list(zip(list(attr_truth.numpy()), list(obj_truth.numpy())))
        seen_ind, unseen_ind = [], []
        for i in range(len(attr_truth)):
            if pairs[i] in self.train_pairs:
                seen_ind.append(i)
            else:
                unseen_ind.append(i)
        seen_ind, unseen_ind = torch.LongTensor(seen_ind), torch.LongTensor(unseen_ind)

        def _process(_scores):
            attr_match = (attr_truth.unsqueeze(1).repeat(1, topk) == _scores[0][:, :topk])
            obj_match = (obj_truth.unsqueeze(1).repeat(1, topk) == _scores[1][:, :topk])
            match = (attr_match * obj_match).any(1).float()
            attr_match = attr_match.any(1).float()
            obj_match = obj_match.any(1).float()
            seen_match = match[seen_ind]
            unseen_match = match[unseen_ind]
            seen_score, unseen_score = torch.ones(512, 5), torch.ones(512, 5)
            return attr_match, obj_match, match, seen_match, unseen_match, torch.Tensor(seen_score + unseen_score), torch.Tensor(seen_score), torch.Tensor(unseen_score)

        def _add_to_dict(_scores, type_name, stats):
            base = ["_attr_match", "_obj_match", "_match", "_seen_match", "_unseen_match", "_ca", "_seen_ca", "_unseen_ca"]
            for val, name in zip(_scores, base):
                stats[type_name + name] = val

        stats = dict()
        closed_scores = _process(predictions["closed"])
        unbiased_closed = _process(predictions["unbiased_closed"])
        _add_to_dict(closed_scores, "closed", stats)
        _add_to_dict(unbiased_closed, "closed_ub", stats)

        scores = predictions["scores"]
        correct_scores = scores[torch.arange(scores.shape[0]), pair_truth][unseen_ind]
        max_seen_scores = predictions['scores'][unseen_ind][:, self.seen_mask].topk(topk, dim=1)[0][:, topk - 1]
        unseen_score_diff = max_seen_scores - correct_scores
        unseen_matches = stats["closed_unseen_match"].bool()
        correct_unseen_score_diff = unseen_score_diff[unseen_matches] - 1e-4
        correct_unseen_score_diff = torch.sort(correct_unseen_score_diff)[0]
        magic_binsize = 20
        bias_skip = max(len(correct_unseen_score_diff) // magic_binsize, 1)
        biaslist = correct_unseen_score_diff[::bias_skip]

        seen_match_max = float(stats["closed_seen_match"].mean())
        unseen_match_max = float(stats["closed_unseen_match"].mean())
        seen_accuracy, unseen_accuracy = [], []

        base_scores = {k: v.to("cpu") for k, v in allpred.items()}
        obj_truth = obj_truth.to("cpu")
        base_scores = torch.stack([allpred[(attr, obj)] for attr, obj in self.dset.pairs], 1)

        for bias in biaslist:
            scores = base_scores.clone()
            results = self.score_fast_model(scores, obj_truth, bias=bias, topk=1)
            results = results['closed']
            results = _process(results)
            seen_match = float(results[3].mean())
            unseen_match = float(results[4].mean())
            seen_accuracy.append(seen_match)
            unseen_accuracy.append(unseen_match)

        seen_accuracy.append(seen_match_max)
        unseen_accuracy.append(unseen_match_max)
        seen_accuracy, unseen_accuracy = np.array(seen_accuracy), np.array(unseen_accuracy)
        area = np.trapz(seen_accuracy, unseen_accuracy)

        for key in stats:
            stats[key] = float(stats[key].mean())

        try:
            harmonic_mean = hmean([seen_accuracy, unseen_accuracy], axis=0)
        except BaseException:
            harmonic_mean = 0

        max_hm = np.max(harmonic_mean)
        idx = np.argmax(harmonic_mean)
        if idx == len(biaslist):
            bias_term = 1e3
        else:
            bias_term = biaslist[idx]
        stats["biasterm"] = float(bias_term)
        stats["best_unseen"] = np.max(unseen_accuracy)
        stats["best_seen"] = np.max(seen_accuracy)
        stats["AUC"] = area
        stats["hm_unseen"] = unseen_accuracy[idx]
        stats["hm_seen"] = seen_accuracy[idx]
        stats["best_hm"] = max_hm
        return stats

# ====================== å®Œå…¨ä¿ç•™åŸæœ‰æ­£ç¡®é€»è¾‘ï¼štestå‡½æ•°ï¼ˆæŒ‡æ ‡æ±‡æ€»ï¼Œä¸€è¡Œæœªæ”¹ï¼‰ ======================
def test(test_dataset, evaluator, all_logits, all_attr_gt, all_obj_gt, all_pair_gt, config):
    predictions = {pair_name: all_logits[:, i] for i, pair_name in enumerate(test_dataset.pairs)}
    all_pred = [predictions]
    all_pred_dict = {}
    for k in all_pred[0].keys():
        all_pred_dict[k] = torch.cat([all_pred[i][k] for i in range(len(all_pred))]).float()
    results = evaluator.score_model(all_pred_dict, all_obj_gt, bias=1e3, topk=1)
    attr_acc = float(torch.mean((results['unbiased_closed'][0].squeeze(-1) == all_attr_gt).float()))
    obj_acc = float(torch.mean((results['unbiased_closed'][1].squeeze(-1) == all_obj_gt).float()))
    stats = evaluator.evaluate_predictions(results, all_attr_gt, all_obj_gt, all_pair_gt, all_pred_dict, topk=1)
    stats['attr_acc'] = attr_acc
    stats['obj_acc'] = obj_acc
    return stats

# ====================== é…ç½®å·¥å…·å‡½æ•°ï¼šçº¯YMLè¯»å–/ä¿å­˜ï¼Œä»…è¦†ç›–è°ƒå‚å‚æ•° ======================
def load_config(cfg_path):
    import yaml
    with open(cfg_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def save_config(config, save_path):
    import yaml
    with open(save_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, sort_keys=False, allow_unicode=True)

def modify_config(param1_val, param2_val):
    """
    ä»…ä¿®æ”¹è°ƒå‚å‚æ•°ï¼Œå…¶ä½™å®Œå…¨ä¿ç•™YMLåŸå§‹é…ç½®
    1. ä»original_cfgæ·±æ‹·è´ï¼Œä¸ä¿®æ”¹åŸæ–‡ä»¶
    2. ä»…å°†ä¸¤ä¸ªè°ƒå‚å‚æ•°å†™å…¥ttaèŠ‚ç‚¹ï¼ˆæ— åˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
    3. ç”Ÿæˆä¸´æ—¶é…ç½®æ–‡ä»¶ï¼Œå®éªŒåè‡ªåŠ¨åˆ é™¤
    """
    cfg = copy.deepcopy(original_cfg)
    # ç¡®ä¿ttaèŠ‚ç‚¹å­˜åœ¨ï¼ˆYMLä¸­æ— åˆ™åˆ›å»ºï¼Œä¸ä¿®æ”¹å…¶ä»–ä»»ä½•èŠ‚ç‚¹ï¼‰
    if "tta" not in cfg:
        cfg["tta"] = {}
    # ä»…è¦†ç›–è°ƒå‚å‚æ•°ï¼Œå…¶ä½™æ‰€æœ‰å‚æ•°ï¼ˆåŒ…æ‹¬use_robust_cache/use_img_cacheç­‰ï¼‰å‡ä»YMLè¯»å–
    cfg["tta"][TUNE_PARAM1] = param1_val
    cfg["tta"][TUNE_PARAM2] = param2_val
    # ç”Ÿæˆä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ˆåŸºäºè°ƒå‚å‚æ•°å‘½åï¼Œé¿å…é‡å¤ï¼‰
    temp_cfg_name = f"temp_tune_{TUNE_PARAM1}_{param1_val:.4f}_{TUNE_PARAM2}_{param2_val:.4f}.yml"
    temp_cfg_path = os.path.join(DIR_PATH, temp_cfg_name)
    save_config(cfg, temp_cfg_path)
    return temp_cfg_path

# ====================== æ ¸å¿ƒåˆå§‹åŒ–ï¼šä»YMLè¯»å–use_robust_cacheï¼Œè‡ªåŠ¨åŒ¹é…è°ƒå‚å‚æ•° ======================
def init_tune_params():
    """
    å…³é”®é€»è¾‘ï¼š
    1. ä»YMLçš„ttaèŠ‚ç‚¹è¯»å–use_robust_cacheï¼ˆæ— åˆ™é»˜è®¤Falseï¼‰
    2. æ ¹æ®use_robust_cacheè‡ªåŠ¨åŒ¹é…å¾…è°ƒå‚çš„2ä¸ªå‚æ•°åŠèŒƒå›´
    3. åˆå§‹åŒ–å…¨å±€è°ƒå‚å˜é‡ï¼Œè®¡ç®—æ€»å®éªŒç»„æ•°
    """
    global original_cfg, use_robust_cache, TUNE_PARAMS, TUNE_PARAM1, TUNE_PARAM2, TUNE_VALS1, TUNE_VALS2, TOTAL_EXP_NUM
    # åŠ è½½åŸå§‹YML
    original_cfg = load_config(CFG_PATH)
    # ä»YMLçš„ttaèŠ‚ç‚¹è¯»å–use_robust_cacheï¼Œå®¹é”™å¤„ç†ï¼ˆæ— åˆ™é»˜è®¤Falseï¼‰
    use_robust_cache = original_cfg.get("tta", {}).get("use_robust_cache", False)
    # è‡ªåŠ¨åŒ¹é…è°ƒå‚å‚æ•°
    if not use_robust_cache:
        # åœºæ™¯1ï¼šä»…æ”¹è¿›ä¸€ â†’ è°ƒlambda_orth + hier_theta
        TUNE_PARAMS = ["lambda_orth", "hier_theta"]
    else:
        # åœºæ™¯2ï¼šæ”¹è¿›ä¸€+äºŒ â†’ è°ƒsim_threshold + correction_interval
        TUNE_PARAMS = ["sim_threshold", "correction_interval"]
    # åˆå§‹åŒ–è°ƒå‚å‚æ•°å˜é‡
    TUNE_PARAM1, TUNE_PARAM2 = TUNE_PARAMS[0], TUNE_PARAMS[1]
    TUNE_VALS1, TUNE_VALS2 = TUNE_PARAMS_SCOPE[TUNE_PARAM1], TUNE_PARAMS_SCOPE[TUNE_PARAM2]
    TOTAL_EXP_NUM = len(TUNE_VALS1) * len(TUNE_VALS2)
    # æ‰“å°è°ƒå‚åœºæ™¯ä¿¡æ¯ï¼ˆæ–¹ä¾¿ç”¨æˆ·æ ¸å¯¹ï¼‰
    print("="*80)
    print("ğŸ“Œ è°ƒå‚åœºæ™¯è‡ªåŠ¨è¯†åˆ«ï¼ˆä»YMLè¯»å–ï¼‰")
    print(f"ğŸ“Œ use_robust_cache: {use_robust_cache}")
    print(f"ğŸ“Œ å¾…è°ƒå‚æ•°ï¼š{TUNE_PARAM1} Ã— {TUNE_PARAM2}")
    print(f"ğŸ“Œ è°ƒå‚èŒƒå›´ï¼š{TUNE_VALS1} Ã— {TUNE_VALS2}")
    print(f"ğŸ“Œ æ€»å®éªŒç»„æ•°ï¼š{TOTAL_EXP_NUM}")
    print("="*80)

# ====================== å®Œå…¨ä¿ç•™åŸæœ‰æ­£ç¡®é€»è¾‘ï¼šrun_experimentï¼ˆå®éªŒè¿è¡Œæ ¸å¿ƒï¼Œä¸€è¡Œæœªæ”¹ï¼‰ ======================
def run_experiment(temp_cfg_path):
    """å®Œå…¨å¤ç”¨swan_test2.pyçš„è¿è¡Œé€»è¾‘ï¼Œæ‰€æœ‰å‚æ•°ä»ä¸´æ—¶YMLè¯»å–"""
    sys.path.append(DIR_PATH)
    from parameters import parser
    from utils import load_args, set_seed
    from dataset import CompositionDataset
    from model.model_factory import get_model
    from swan_test_hitomcat import predict_logits_text_first_with_hitomcat  # ä½ çš„æ”¹è¿›é¢„æµ‹å‡½æ•°

    try:
        # 1. é…ç½®è§£æï¼ˆå’Œswan_test2.pyå®Œå…¨ä¸€è‡´ï¼‰
        args = parser.parse_args(["--cfg", temp_cfg_path])
        load_args(args.cfg, args)
        config = args
        # éšæœºç§å­ä»YMLè¯»å–ï¼Œæ— åˆ™ç”¨BASE_SEED
        try:
            set_seed(config.seed)
        except AttributeError:
            set_seed(BASE_SEED)
        config.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"  ğŸ“Œ ä½¿ç”¨è®¾å¤‡ï¼š{config.device}")

        # 2. å®ä¾‹åŒ–æµ‹è¯•æ•°æ®é›†ï¼ˆæ‰€æœ‰å‚æ•°ä»YMLè¯»å–ï¼‰
        print(f"  ğŸ“Œ åŠ è½½æ•°æ®é›†ï¼š{config.dataset}")
        test_dataset = CompositionDataset(
            config.dataset_path,
            phase='test',
            split='compositional-split-natural',
            open_world=config.open_world
        )

        # 3. æ¨¡å‹åŠ è½½ï¼ˆæ‰€æœ‰å‚æ•°ä»YMLè¯»å–ï¼Œå¯¹é½swan_test2.pyï¼‰
        print(f"  ğŸ“Œ åŠ è½½æ¨¡å‹ï¼š{config.load_model}")
        allattrs = test_dataset.attrs
        allobj = test_dataset.objs
        classes = [cla.replace(".", " ").lower() for cla in allobj]
        attributes = [attr.replace(".", " ").lower() for attr in allattrs]
        offset = len(attributes)
        model = get_model(config, attributes=attributes, classes=classes, offset=offset).to(config.device)
        if config.load_model and os.path.exists(config.load_model):
            model.load_state_dict(torch.load(config.load_model, map_location='cpu'))
        model.eval()

        # 4. è¿è¡Œé¢„æµ‹ï¼ˆå¸¦autocastï¼Œå’Œswan_test2.pyå®Œå…¨ä¸€è‡´ï¼‰
        print(f"  ğŸ“Œ å¼€å§‹é¢„æµ‹...")
        with autocast(dtype=torch.bfloat16):
            all_logits, all_attr_gt, all_obj_gt, all_pair_gt = predict_logits_text_first_with_hitomcat(model, test_dataset, config)

        # 5. è®¡ç®—æŒ‡æ ‡ï¼ˆå¤ç”¨åŸæœ‰æ­£ç¡®é€»è¾‘ï¼‰
        print(f"  ğŸ“Œ è®¡ç®—æŒ‡æ ‡...")
        evaluator = Evaluator(test_dataset, model=None, device=config.device)
        test_stats = test(test_dataset, evaluator, all_logits, all_attr_gt, all_obj_gt, all_pair_gt, config)

        # æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
        if os.path.exists(temp_cfg_path):
            os.remove(temp_cfg_path)

        # æå–æ ¸å¿ƒæŒ‡æ ‡ï¼Œä¿è¯æ— ç¼ºå¤±
        res = {k: test_stats.get(k, 0.0) for k in CORE_METRICS}
        print(f"  âœ… å®éªŒå®Œæˆ | AUC: {res['AUC']:.4f} | Best HM: {res['best_hm']:.4f}")
        return res

    except Exception as e:
        # å®éªŒå¤±è´¥æ—¶å¼ºåˆ¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_cfg_path):
            os.remove(temp_cfg_path)
        raise Exception(f"è¿è¡Œå¼‚å¸¸ï¼š{str(e)}")

# ====================== æ•°æ®è®°å½•å·¥å…·ï¼šè‡ªé€‚åº”è°ƒå‚å‚æ•°ï¼ŒCSV+SwanLab ======================
def init_swanlab(project_name):
    """åˆå§‹åŒ–SwanLabï¼Œè®°å½•YMLä¸­çš„æ ¸å¿ƒé…ç½®å’Œè°ƒå‚ä¿¡æ¯"""
    swanlab.init(
        project=project_name,
        config={
            "cfg_path": CFG_PATH,
            "use_robust_cache": use_robust_cache,
            "tune_params": TUNE_PARAMS,
            "total_exp_num": TOTAL_EXP_NUM
        },
        log_level="info",
        mode="online" if original_cfg.get("use_wandb", True) else "offline"  # wandbæ¨¡å¼ä»YMLè¯»å–
    )

def init_csv(save_dir, core_metrics):
    """åˆå§‹åŒ–CSVï¼Œè¡¨å¤´è‡ªåŠ¨é€‚é…å½“å‰è°ƒå‚å‚æ•°"""
    os.makedirs(save_dir, exist_ok=True)
    # æ–‡ä»¶åæ ‡è®°è°ƒå‚åœºæ™¯ï¼Œé¿å…è¦†ç›–
    csv_suffix = "robustcache_on" if use_robust_cache else "robustcache_off"
    csv_path = os.path.join(save_dir, f"tune_metrics_{csv_suffix}.csv")
    # è¡¨å¤´ï¼šè°ƒå‚å‚æ•°1 + è°ƒå‚å‚æ•°2 + æ ¸å¿ƒæŒ‡æ ‡
    headers = [TUNE_PARAM1, TUNE_PARAM2] + core_metrics
    with open(csv_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
    return csv_path

def record_data(csv_path, param1_val, param2_val, metrics):
    """è®°å½•å•æ¬¡å®éªŒæ•°æ®ï¼ŒåŠ¨æ€é€‚é…è°ƒå‚å‚æ•°"""
    row = {TUNE_PARAM1: param1_val, TUNE_PARAM2: param2_val}
    row.update(metrics)
    # å†™å…¥CSV
    with open(csv_path, 'a+', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=list(row.keys()))
        writer.writerow(row)
    # åŒæ­¥åˆ°SwanLabï¼ˆå«è°ƒå‚å‚æ•°ï¼Œæ–¹ä¾¿åœ¨çº¿åˆ†æï¼‰
    swanlab.log({**metrics, TUNE_PARAM1: param1_val, TUNE_PARAM2: param2_val})

# ====================== å¯è§†åŒ–å·¥å…·ï¼šè‡ªé€‚åº”è°ƒå‚å‚æ•°ï¼Œé²æ£’æ€§å¼º ======================
def visualize_results(save_dir, csv_path, core_metrics):
    import pandas as pd
    import json
    import matplotlib.pyplot as plt
    import os  # è¡¥å……ç¼ºå¤±çš„oså¯¼å…¥ï¼ŒåŸä»£ç ç”¨åˆ°äº†os.pathå´æ²¡å¯¼å…¥
    
    # åŠ è½½å¹¶è¿‡æ»¤æœ‰æ•ˆæ•°æ®ï¼ˆå‰”é™¤AUC<=0/ç©ºå€¼çš„æ— æ•ˆå®éªŒï¼‰
    df = pd.read_csv(csv_path)
    df = df.dropna(subset=["AUC"])
    df = df[df["AUC"] > 0]
    if len(df) == 0:
        print("ã€è­¦å‘Šã€‘æ— æœ‰æ•ˆå®éªŒæ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–ï¼")
        return

    # åœºæ™¯åç¼€ï¼Œç”¨äºæ–‡ä»¶å
    csv_suffix = "robustcache_on" if use_robust_cache else "robustcache_off"

    # 1. å¤šæŒ‡æ ‡çƒ­åŠ›å›¾ï¼ˆAUC/best_hm/attr_accï¼‰ï¼Œä¿®å¤åˆ»åº¦+param_names+osç¼ºå¤±é—®é¢˜
    for value in ["AUC", "best_hm", "attr_acc"]:
        plt.figure(figsize=(10,8))
        # æ ¸å¿ƒä¿®å¤ï¼šç”¨å…¨å±€TUNE_PARAM1/TUNE_PARAM2æ›¿æ¢æœªå®šä¹‰çš„param_names
        pivot = df.pivot(index=TUNE_PARAM1, columns=TUNE_PARAM2, values=value)
        # ã€å¯é€‰ä¼˜åŒ–ã€‘æŒ‰æ•°å€¼æ’åºpivotçš„è¡Œåˆ—ï¼Œè®©çƒ­åŠ›å›¾æŒ‰å‚æ•°å¤§å°é¡ºåºå±•ç¤ºï¼ˆé¿å…ä¹±åºï¼‰
        pivot = pivot.sort_index(ascending=True).sort_index(axis=1, ascending=True)
        im = plt.imshow(pivot, cmap="YlGnBu", aspect="auto")
        
        # ===================== æ ¸å¿ƒä¿®æ”¹ï¼šè®¾ç½®å®é™…æ•°å€¼åˆ»åº¦ =====================
        # xè½´ï¼šåˆ»åº¦ä½ç½®=åˆ—ç´¢å¼•ï¼Œåˆ»åº¦æ ‡ç­¾=pivotåˆ—çš„å®é™…å‚æ•°å€¼ï¼ˆä¿ç•™4ä½å°æ•°ï¼Œå¯æŒ‰éœ€ä¿®æ”¹ï¼‰
        plt.xticks(
            range(len(pivot.columns)),  # åˆ»åº¦ä½ç½®ï¼š0,1,2...
            [f"{x:.4f}" for x in pivot.columns],  # åˆ»åº¦æ ‡ç­¾ï¼šå®é™…å‚æ•°å€¼
            fontsize=10, 
            rotation=45  # æ—‹è½¬45åº¦ï¼Œé¿å…æ ‡ç­¾é‡å ï¼ˆå¯æ ¹æ®éœ€è¦æ”¹0/30/60ï¼‰
        )
        # yè½´ï¼šåˆ»åº¦ä½ç½®=è¡Œç´¢å¼•ï¼Œåˆ»åº¦æ ‡ç­¾=pivotè¡Œçš„å®é™…å‚æ•°å€¼
        plt.yticks(
            range(len(pivot.index)), 
            [f"{x:.4f}" for x in pivot.index], 
            fontsize=10
        )
        # =====================================================================
        
        # æ•°å€¼æ ‡æ³¨ï¼Œä¿ç•™4ä½å°æ•°
        for i in range(len(pivot.index)):
            for j in range(len(pivot.columns)):
                plt.text(j, i, f"{pivot.iloc[i,j]:.4f}", ha="center", va="center", fontsize=10)
        plt.colorbar(im, label=value)
        # ä¿®å¤ï¼šå›¾è¡¨è½´æ ‡ç­¾æ›¿æ¢ä¸ºå®é™…è°ƒå‚å‚æ•°å
        plt.xlabel(TUNE_PARAM2, fontsize=14, fontweight="bold")
        plt.ylabel(TUNE_PARAM1, fontsize=14, fontweight="bold")
        plt.title(f"{value} Heatmap (Higher is Better)", fontsize=16, fontweight="bold")
        plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé€‚é…æ—‹è½¬åçš„æ ‡ç­¾
        plt.savefig(os.path.join(save_dir, f"{value}_heatmap_{csv_suffix}.png"), dpi=300)
        plt.close()

    # 2. æ ¸å¿ƒæŒ‡æ ‡æŠ˜çº¿å›¾ï¼ˆAUC/best_hm/attr_accï¼‰ï¼ŒåŸæœ‰é€»è¾‘ä¸å˜
    for metric in ["AUC", "best_hm", "attr_acc"]:
        plt.figure(figsize=(12, 6))
        for param2_val in TUNE_VALS2:
            param2_data = df[df[TUNE_PARAM2] == param2_val].sort_values(TUNE_PARAM1)
            if len(param2_data) == 0:
                continue
            plt.plot(param2_data[TUNE_PARAM1], param2_data[metric], marker="o", linewidth=2, label=f"{TUNE_PARAM2}={param2_val}")
        # è‡ªé€‚åº”åæ ‡è½´
        plt.xlabel(TUNE_PARAM1, fontsize=14, fontweight="bold")
        plt.ylabel(metric, fontsize=14, fontweight="bold")
        plt.title(f"Improve-Tune: {metric} vs {TUNE_PARAM1} (use_robust_cache={use_robust_cache})", fontsize=16, fontweight="bold")
        plt.legend(loc="best", fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, f"{metric}_lineplot_{csv_suffix}.png"), dpi=300)
        plt.close()

    # 3. ä¿å­˜æœ€ä¼˜å‚æ•°ï¼ˆæŒ‰AUCæœ€å¤§åŒ–ï¼Œè‡ªé€‚åº”è°ƒå‚å‚æ•°ï¼‰ï¼ŒåŸæœ‰é€»è¾‘ä¸å˜
    best_idx = df["AUC"].idxmax()
    best_row = df.loc[best_idx]
    best_params = {
        f"best_{TUNE_PARAM1}": float(best_row[TUNE_PARAM1]),
        f"best_{TUNE_PARAM2}": float(best_row[TUNE_PARAM2]),
        **{k: float(best_row[k]) for k in core_metrics}
    }
    # ä¿å­˜æœ€ä¼˜å‚æ•°åˆ°JSONï¼Œå…¨é‡è°ƒå‚æ•°æ®åˆ°Excel
    best_param_path = os.path.join(save_dir, f"best_params_{csv_suffix}.json")
    excel_path = os.path.join(save_dir, f"tune_metrics_{csv_suffix}.xlsx")
    with open(best_param_path, 'w', encoding='utf-8') as f:
        json.dump(best_params, f, indent=4)
    df.to_excel(excel_path, index=False)

    # é†’ç›®æ‰“å°æœ€ä¼˜å‚æ•°åŠæ ¸å¿ƒæŒ‡æ ‡ï¼Œæ§åˆ¶å°ç›´è§‚å±•ç¤º
    print("\n" + "="*80)
    print(f"âœ… è°ƒå‚å®Œæˆï¼use_robust_cache={use_robust_cache} æœ€ä¼˜å‚æ•°å¦‚ä¸‹ï¼š")
    print("="*80)
    print(f"ğŸ“Œ æœ€ä¼˜{TUNE_PARAM1}ï¼š{best_params[f'best_{TUNE_PARAM1}']:.4f}")
    print(f"ğŸ“Œ æœ€ä¼˜{TUNE_PARAM2}ï¼š{best_params[f'best_{TUNE_PARAM2}']:.4f}")
    print("-"*80)
    for k in core_metrics:
        if k in ["AUC", "best_hm", "biasterm"]:
            print(f"ğŸ“Š {k:12s}ï¼š{best_params[k]:.4f}")
        else:
            print(f"ğŸ“Š {k:12s}ï¼š{best_params[k]:.2%}")
    print("="*80)
    print(f"ğŸ“ æ‰€æœ‰è°ƒå‚ç»“æœå·²ä¿å­˜è‡³ï¼š{os.path.abspath(save_dir)}")
# ====================== ä¸»å‡½æ•°ï¼šç½‘æ ¼æœç´¢ä¸»æµç¨‹ï¼ˆå…¨è‡ªåŠ¨ï¼Œæ— äººå·¥å¹²é¢„ï¼‰ ======================
def main():
    # ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–è°ƒå‚å‚æ•°ï¼ˆä»YMLè¯»å–ï¼Œè‡ªåŠ¨åŒ¹é…ï¼‰
    init_tune_params()
    # ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–æ•°æ®è®°å½•ï¼ˆCSV+SwanLabï¼‰
    csv_path = init_csv(SAVE_DIR, CORE_METRICS)
    init_swanlab(SWANLAB_PROJECT)

    # ç¬¬ä¸‰æ­¥ï¼šç½‘æ ¼æœç´¢éå†æ‰€æœ‰å‚æ•°ç»„åˆ
    param_combinations = product(TUNE_VALS1, TUNE_VALS2)
    success_num = 0
    print(f"\nğŸš€ å¼€å§‹ç½‘æ ¼æœç´¢è°ƒå‚ï¼Œæ€»{TOTAL_EXP_NUM}ç»„å®éªŒ...")

    for idx, (param1_val, param2_val) in enumerate(param_combinations, 1):
        print(f"\n{'='*60}")
        print(f"ã€å®éªŒ {idx}/{TOTAL_EXP_NUM}ã€‘{TUNE_PARAM1}={param1_val:.4f}, {TUNE_PARAM2}={param2_val:.4f}")
        print(f"{'='*60}")
        try:
            # ç”Ÿæˆä¸´æ—¶YMLï¼ˆä»…è¦†ç›–è°ƒå‚å‚æ•°ï¼‰
            temp_cfg_path = modify_config(param1_val, param2_val)
            # è¿è¡Œå•æ¬¡å®éªŒ
            metrics = run_experiment(temp_cfg_path)
            # è®°å½•æ•°æ®
            record_data(csv_path, param1_val, param2_val, metrics)
            success_num += 1
        except Exception as e:
            print(f"âŒ å®éªŒå¤±è´¥ | é”™è¯¯è¯¦æƒ…ï¼š{e}")
            continue

    # ç¬¬å››æ­¥ï¼šå¯è§†åŒ–ç»“æœ+ä¿å­˜æœ€ä¼˜å‚æ•°
    visualize_results(SAVE_DIR, csv_path, CORE_METRICS)
    # ç»“æŸSwanLabæ—¥å¿—
    swanlab.finish()

    # å®éªŒæ€»ç»“
    print(f"\nğŸ“Š è°ƒå‚å®éªŒæ€»ç»“ï¼š")
    print(f"ğŸ“Œ æ€»å®éªŒç»„æ•°ï¼š{TOTAL_EXP_NUM} | æˆåŠŸï¼š{success_num} | å¤±è´¥ï¼š{TOTAL_EXP_NUM-success_num}")
    if success_num == 0:
        print("âŒ æ‰€æœ‰å®éªŒå¤±è´¥ï¼Œè¯·ä¼˜å…ˆæ£€æŸ¥YMLä¸­çš„ã€dataset_path/load_modelã€‘è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼")

if __name__ == "__main__":
    main()