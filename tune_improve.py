import os
import sys
import csv
import json
import copy
import random
import torch
import numpy as np
import matplotlib.pyplot as plt
import swanlab
from itertools import product
from datetime import datetime
from torch.cuda.amp import autocast

# ====================== å…¨å±€é…ç½®ï¼šä»…æŒ‡å®šã€å½“å‰é˜¶æ®µè¦è°ƒçš„2ä¸ªå‚æ•°èŒƒå›´ã€‘+ åŸºç¡€é€‚é… ======================
plt.switch_backend('Agg')
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ---------------------- ä»…éœ€ä¿®æ”¹è¿™é‡Œï¼æŒ‡å®šå½“å‰è¦è°ƒçš„2ä¸ªå‚æ•°èŒƒå›´ ----------------------
# æ³¨æ„ï¼šè°ƒå‚èŒƒå›´äºŒé€‰ä¸€ï¼Œæ ¹æ®ymlä¸­use_robust_cacheå¼€å…³åŒ¹é…
# 1. STAGE1ï¼ˆymlä¸­use_robust_cache=Falseï¼‰ï¼šä»…æ”¹è¿›ä¸€ï¼Œè°ƒä»¥ä¸‹2ä¸ªå‚æ•°
TUNE_PARAMS_STAGE1 = {
    "param_names": ["lambda_orth", "hier_theta"],
    "ranges": [
        [0.01,0.05,0.1,0.4,0.7,1.0,1.3,1.6,2],  # lambda_orthèŒƒå›´
        [0.9, 1.0, 1.1]                        # hier_thetaèŒƒå›´
    ]
}

# 2. STAGE2ï¼ˆymlä¸­use_robust_cache=Trueï¼‰ï¼šæ”¹è¿›ä¸€+äºŒï¼Œè°ƒä»¥ä¸‹2ä¸ªå‚æ•°ï¼ˆæ”¹è¿›ä¸€å›ºå®šï¼‰
TUNE_PARAMS_STAGE2 = {
    "param_names": ["correction_interval", "sim_threshold"],
    "ranges": [
        [10, 20, 30, 40],                          # correction_intervalèŒƒå›´
        [0.10, 0.15, 0.20, 0.25, 0.30]             # sim_thresholdèŒƒå›´
    ],
    "improve1_best_params_path": "tune_improve1_only_results/best_params.json"  # é˜¶æ®µ1æœ€ä¼˜å‚æ•°è·¯å¾„
}

# ---------------------- å›ºå®šé…ç½®ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰ ----------------------
CFG_PATH = "config/ut-zappos.yml"  # ä½ çš„ymlè·¯å¾„
CORE_METRICS = ["AUC", "best_hm", "attr_acc", "best_seen", "best_unseen", "obj_acc", "biasterm"]
SAVE_DIR_PREFIX = "tune_results_"  # ç»“æœä¿å­˜ç›®å½•å‰ç¼€ï¼ˆè‡ªåŠ¨åŠ é˜¶æ®µåï¼‰

# ====================== å¯¹é½swan_test2.pyçš„å·¥å…·ç±»ï¼ˆæ— ä¿®æ”¹ï¼‰======================
DIR_PATH = os.path.dirname(os.path.abspath(__file__))
# class Evaluator:
#     def __init__(self, dset, model, device):
#         self.dset = dset
#         self.device = device
#         pairs = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in dset.pairs]
#         self.train_pairs = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in dset.train_pairs]
#         self.pairs = torch.LongTensor(pairs)

#         if dset.phase == 'train':
#             test_pair_set = set(dset.train_pairs)
#             test_pair_gt = set(dset.train_pairs)
#         elif dset.phase == 'val':
#             test_pair_set = set(dset.val_pairs + dset.train_pairs)
#             test_pair_gt = set(dset.val_pairs)
#         else:
#             test_pair_set = set(dset.test_pairs + dset.train_pairs)
#             test_pair_gt = set(dset.test_pairs)

#         self.test_pair_dict = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in test_pair_gt]
#         self.test_pair_dict = dict.fromkeys(self.test_pair_dict, 0)
#         for attr, obj in test_pair_gt:
#             pair_val = dset.pair2idx[(attr, obj)]
#             key = (dset.attr2idx[attr], dset.obj2idx[obj])
#             self.test_pair_dict[key] = [pair_val, 0, 0]

#         masks = [1 for _ in dset.pairs] if dset.open_world else [1 if pair in test_pair_set else 0 for pair in dset.pairs]
#         self.closed_mask = torch.BoolTensor(masks)
#         seen_mask = [1 if pair in set(dset.train_pairs) else 0 for pair in dset.pairs]
#         self.seen_mask = torch.BoolTensor(seen_mask)

#         oracle_obj_mask = []
#         for _obj in dset.objs:
#             oracle_obj_mask.append(torch.BoolTensor([1 if _obj == obj else 0 for attr, obj in dset.pairs]))
#         self.oracle_obj_mask = torch.stack(oracle_obj_mask, 0)
#         self.score_model = self.score_manifold_model

#     def generate_predictions(self, scores, obj_truth, bias=0.0, topk=1):
#         def get_pred(_s):
#             _, pred = _s.topk(topk, dim=1)
#             pred = pred.view(-1)
#             return self.pairs[pred][:,0].view(-1,topk), self.pairs[pred][:,1].view(-1,topk)
#         orig = scores.clone()
#         scores[~self.seen_mask.repeat(scores.shape[0],1)] += bias
#         return {
#             "open": get_pred(scores),
#             "unbiased_open": get_pred(orig),
#             "closed": get_pred(scores.masked_fill(~self.closed_mask.repeat(scores.shape[0],1), -1e10)),
#             "unbiased_closed": get_pred(orig.masked_fill(~self.closed_mask.repeat(scores.shape[0],1), -1e10))
#         }

#     def score_manifold_model(self, scores, obj_truth, bias=0.0, topk=1):
#         scores = torch.stack([scores[(a,o)] for a,o in self.dset.pairs], 1)
#         return {**self.generate_predictions(scores, obj_truth, bias, topk), "scores": scores.clone()}

#     def evaluate_predictions(self, preds, attr_gt, obj_gt, pair_gt, allpred, topk=1):
#         from scipy.stats import hmean
#         attr_gt, obj_gt, pair_gt = attr_gt.cpu(), obj_gt.cpu(), pair_gt.cpu()
#         seen_ind = torch.tensor([i for i,(a,o) in enumerate(zip(attr_gt.numpy(), obj_gt.numpy())) if (a,o) in self.train_pairs])
#         unseen_ind = torch.tensor([i for i,(a,o) in enumerate(zip(attr_gt.numpy(), obj_gt.numpy())) if (a,o) not in self.train_pairs])

#         def process(s):
#             a_match = (attr_gt.unsqueeze(1).repeat(1,topk) == s[0][:,:topk]).any(1).float()
#             o_match = (obj_gt.unsqueeze(1).repeat(1,topk) == s[1][:,:topk]).any(1).float()
#             match = (a_match * o_match).float()
#             return a_match, o_match, match, match[seen_ind], match[unseen_ind]
#         stats = {}
#         for k in ["closed", "unbiased_closed"]:
#             a,o,m,s,u = process(preds[k])
#             stats[f"{k}_attr_match"] = a.mean().item()
#             stats[f"{k}_obj_match"] = o.mean().item()
#             stats[f"{k}_match"] = m.mean().item()
#             stats[f"{k}_seen_match"] = s.mean().item() if len(s) else 0.0
#             stats[f"{k}_unseen_match"] = u.mean().item() if len(u) else 0.0

#         scores = preds["scores"]
#         correct_scores = scores[torch.arange(len(scores)), pair_gt][unseen_ind]
#         max_seen = scores[unseen_ind][:, self.seen_mask].topk(topk,1)[0][:,topk-1]
#         diff = max_seen - correct_scores
#         valid_diff = diff[stats["closed_unseen_match"]>0] - 1e-4
#         biaslist = valid_diff[::max(len(valid_diff)//20,1)] if len(valid_diff) else [0.0]

#         seen_acc, unseen_acc = [stats["closed_seen_match"]], [stats["closed_unseen_match"]]
#         base_scores = torch.stack([allpred[(a,o)] for a,o in self.dset.pairs], 1)
#         for b in biaslist:
#             s,u = process(self.score_fast_model(base_scores.clone(), obj_gt, b, topk))[3:]
#             seen_acc.append(s.mean().item() if len(s) else 0.0)
#             unseen_acc.append(u.mean().item() if len(u) else 0.0)

#         seen_acc, unseen_acc = np.array(seen_acc), np.array(unseen_acc)
#         hm = hmean([seen_acc, unseen_acc], axis=0) if len(seen_acc) else 0.0
#         return {
#             **stats,
#             "AUC": np.trapz(seen_acc, unseen_acc),
#             "best_hm": np.max(hm) if len(hm) else 0.0,
#             "best_seen": np.max(seen_acc),
#             "best_unseen": np.max(unseen_acc),
#             "biasterm": biaslist[np.argmax(hm)] if len(hm) else 1e3
#         }

#     def score_fast_model(self, scores, obj_truth, bias=0.0, topk=1):
#         scores[~self.seen_mask.repeat(scores.shape[0],1)] += bias
#         closed = scores.masked_fill(~self.closed_mask.repeat(scores.shape[0],1), -1e10)
#         _, pred = closed.topk(topk,1)
#         pred = pred.view(-1)
#         return (self.pairs[pred][:,0].view(-1,topk), self.pairs[pred][:,1].view(-1,topk))
# ====================== å¯¹é½swan_test2.pyçš„å·¥å…·ç±»ï¼ˆç»ˆæä¿®å¤ï¼šè§£å†³unseenè®¡ç®—ç»´åº¦å±•å¹³ï¼‰======================
DIR_PATH = os.path.dirname(os.path.abspath(__file__))
class Evaluator:
    def __init__(self, dset, model, device):
        self.dset = dset
        self.device = device
        pairs = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in dset.pairs]
        self.train_pairs = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in dset.train_pairs]
        self.pairs = torch.LongTensor(pairs)

        if dset.phase == 'train':
            test_pair_set = set(dset.train_pairs)
            test_pair_gt = set(dset.train_pairs)
        elif dset.phase == 'val':
            test_pair_set = set(dset.val_pairs + dset.train_pairs)
            test_pair_gt = set(dset.val_pairs)
        else:
            test_pair_set = set(dset.test_pairs + dset.train_pairs)
            test_pair_gt = set(dset.test_pairs)

        self.test_pair_dict = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in test_pair_gt]
        self.test_pair_dict = dict.fromkeys(self.test_pair_dict, 0)
        for attr, obj in test_pair_gt:
            pair_val = dset.pair2idx[(attr, obj)]
            key = (dset.attr2idx[attr], dset.obj2idx[obj])
            self.test_pair_dict[key] = [pair_val, 0, 0]

        masks = [1 for _ in dset.pairs] if dset.open_world else [1 if pair in test_pair_set else 0 for pair in dset.pairs]
        self.closed_mask = torch.BoolTensor(masks)
        seen_mask = [1 if pair in set(dset.train_pairs) else 0 for pair in dset.pairs]
        self.seen_mask = torch.BoolTensor(seen_mask)
        # é¢„è®¡ç®—seen pairçš„æ•°é‡ï¼ˆç”¨äºç»´åº¦æ ¡éªŒï¼‰
        self.seen_pair_num = self.seen_mask.sum().item()
        print(f"ã€Evaluatoråˆå§‹åŒ–ã€‘seen_maské•¿åº¦ï¼š{len(self.seen_mask)} | seen pairæ•°ï¼š{self.seen_pair_num}")

        oracle_obj_mask = []
        for _obj in dset.objs:
            oracle_obj_mask.append(torch.BoolTensor([1 if _obj == obj else 0 for attr, obj in dset.pairs]))
        self.oracle_obj_mask = torch.stack(oracle_obj_mask, 0)
        self.score_model = self.score_manifold_model

    def generate_predictions(self, scores, obj_truth, bias=0.0, topk=1):
        def get_pred(_s):
            _, pred = _s.topk(topk, dim=1)
            pred = pred.view(-1)
            return self.pairs[pred][:,0].view(-1,topk), self.pairs[pred][:,1].view(-1,topk)
        orig = scores.clone()
        scores[~self.seen_mask.repeat(scores.shape[0],1)] += bias
        return {
            "open": get_pred(scores),
            "unbiased_open": get_pred(orig),
            "closed": get_pred(scores.masked_fill(~self.closed_mask.repeat(scores.shape[0],1), -1e10)),
            "unbiased_closed": get_pred(orig.masked_fill(~self.closed_mask.repeat(scores.shape[0],1), -1e10))
        }

    def score_manifold_model(self, scores, obj_truth, bias=0.0, topk=1):
        scores = torch.stack([scores[(a,o)] for a,o in self.dset.pairs], 1)
        # ç»´åº¦æ ¡éªŒ
        assert scores.shape == (len(obj_truth), len(self.dset.pairs)), \
            f"score_manifold_model: scoresç»´åº¦å¼‚å¸¸ {scores.shape}ï¼Œé¢„æœŸ({len(obj_truth)}, {len(self.dset.pairs)})"
        return {**self.generate_predictions(scores, obj_truth, bias, topk), "scores": scores.clone()}

    def evaluate_predictions(self, preds, attr_gt, obj_gt, pair_gt, allpred, topk=1):
        from scipy.stats import hmean
        attr_gt, obj_gt, pair_gt = attr_gt.cpu(), obj_gt.cpu(), pair_gt.cpu()
        # ä¼˜åŒ–seen/unseenç´¢å¼•è®¡ç®—ï¼Œé¿å…å¾ªç¯ï¼Œæå‡é€Ÿåº¦+ç¨³å®šæ€§
        pair_comb = torch.stack([attr_gt, obj_gt], dim=1).numpy()
        train_pair_set = set(tuple(p) for p in self.train_pairs)
        seen_mask = np.array([tuple(p) in train_pair_set for p in pair_comb])
        seen_ind = torch.where(torch.BoolTensor(seen_mask))[0]
        unseen_ind = torch.where(~torch.BoolTensor(seen_mask))[0]
        self.unseen_num = len(unseen_ind)
        print(f"ã€evaluate_predictionsã€‘æ€»æ ·æœ¬ï¼š{len(attr_gt)} | seenæ ·æœ¬ï¼š{len(seen_ind)} | unseenæ ·æœ¬ï¼š{self.unseen_num}")

        def process(s):
            a_match = (attr_gt.unsqueeze(1).repeat(1,topk) == s[0][:,:topk]).any(1).float()
            o_match = (obj_gt.unsqueeze(1).repeat(1,topk) == s[1][:,:topk]).any(1).float()
            match = (a_match * o_match).float()
            return a_match, o_match, match, match[seen_ind], match[unseen_ind]

        stats = {}
        for k in ["closed", "unbiased_closed"]:
            a,o,m,s,u = process(preds[k])
            stats[f"{k}_attr_match"] = a.mean().item()
            stats[f"{k}_obj_match"] = o.mean().item()
            stats[f"{k}_match"] = m.mean().item()
            stats[f"{k}_seen_match"] = s.mean().item() if len(s) else 0.0
            stats[f"{k}_unseen_match"] = u.mean().item() if len(u) else 0.0

        scores = preds["scores"]
        # ====================== æ ¸å¿ƒä¿®å¤ï¼šunseenæ ·æœ¬scoresè®¡ç®—ï¼ˆè§£å†³96162ç»´åº¦å±•å¹³ï¼‰======================
        if self.unseen_num == 0:
            biaslist = [0.0]
            print("ã€unseenè®¡ç®—ã€‘æ— unseenæ ·æœ¬ï¼Œè·³è¿‡biasè®¡ç®—")
        else:
            # 1. è®¡ç®—unseenæ ·æœ¬çš„æ­£ç¡®pairå¾—åˆ†ï¼ˆç»´åº¦[1891]ï¼Œå¼ºåˆ¶ä¸€ç»´ï¼‰
            correct_scores = scores[torch.arange(len(scores)), pair_gt][unseen_ind].squeeze()
            # å¼ºåˆ¶reshapeä¸ºä¸€ç»´ï¼Œé¿å…éšæ€§ç»´åº¦é—®é¢˜
            correct_scores = correct_scores.reshape(-1)
            print(f"ã€unseenè®¡ç®—ã€‘correct_scoresç»´åº¦ï¼š{correct_scores.shape}ï¼ˆé¢„æœŸ[{self.unseen_num}]ï¼‰")

            # 2. è®¡ç®—unseenæ ·æœ¬çš„seen pairæœ€å¤§å¾—åˆ†ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šé¿å…å±•å¹³ï¼Œå¼ºåˆ¶ä¸€ç»´ï¼‰
            # å…ˆç´¢å¼•unseenæ ·æœ¬ï¼Œå†å–seen maskï¼Œå¾—åˆ°[1891,33]
            scores_unseen_seen = scores[unseen_ind][:, self.seen_mask]
            print(f"ã€unseenè®¡ç®—ã€‘scores_unseen_seenç»´åº¦ï¼š{scores_unseen_seen.shape}ï¼ˆé¢„æœŸ[{self.unseen_num},{self.seen_pair_num}]ï¼‰")
            # topkå–æœ€å¤§å€¼ï¼Œå¾—åˆ°[1891,1]ï¼Œå†squeeze+reshapeä¸º[1891]
            max_seen, _ = scores_unseen_seen.topk(topk, dim=1)
            max_seen = max_seen.squeeze(dim=1).reshape(-1)
            print(f"ã€unseenè®¡ç®—ã€‘max_seenç»´åº¦ï¼š{max_seen.shape}ï¼ˆé¢„æœŸ[{self.unseen_num}]ï¼‰")

            # 3. ç»´åº¦å¼ºåˆ¶æ ¡éªŒï¼ˆæ ¸å¿ƒï¼ç¡®ä¿ä¸¤ä¸ªå¼ é‡éƒ½æ˜¯[1891]ï¼‰
            assert correct_scores.shape == max_seen.shape == (self.unseen_num,), \
                f"ç»´åº¦ä¸åŒ¹é…ï¼šcorrect_scores{correct_scores.shape} | max_seen{max_seen.shape}ï¼Œé¢„æœŸå‡ä¸º({self.unseen_num},)"

            # 4. è®¡ç®—å·®å€¼ï¼Œåç»­æ“ä½œå‡åŸºäºä¸€ç»´å¼ é‡
            diff = max_seen - correct_scores
            diff = diff.reshape(-1)
            print(f"ã€unseenè®¡ç®—ã€‘diffç»´åº¦ï¼š{diff.shape}ï¼ˆé¢„æœŸ[{self.unseen_num}]ï¼‰")

            # 5. è¿‡æ»¤æœ‰æ•ˆå·®å€¼ï¼ˆä¿®å¤maskå¹¿æ’­é”™è¯¯ï¼ŒåŸä»£ç ç”¨æ ‡é‡ç´¢å¼•çš„bugï¼‰
            # åŸé”™è¯¯ï¼šstats["closed_unseen_match"]æ˜¯æ ‡é‡ï¼Œç”¨æ ‡é‡ç´¢å¼•ä¼šå¯¼è‡´å¹¿æ’­
            # æ­£ç¡®ï¼šå–unseenæ ·æœ¬çš„matchç»“æœï¼Œç”Ÿæˆmask
            unseen_match = process(preds["closed"])[4]  # å–unseen_indçš„matchç»“æœ
            valid_mask = (unseen_match > 0).cpu()
            valid_diff = diff[valid_mask] - 1e-4
            valid_diff = valid_diff.reshape(-1)
            print(f"ã€unseenè®¡ç®—ã€‘valid_diffç»´åº¦ï¼š{valid_diff.shape} | æœ‰æ•ˆæ ·æœ¬æ•°ï¼š{len(valid_diff)}")

            # 6. ç”Ÿæˆbiaslistï¼ˆé¿å…æ­¥é•¿å¯¼è‡´çš„ç»´åº¦è†¨èƒ€ï¼‰
            if len(valid_diff) == 0:
                biaslist = [0.0]
            else:
                step = max(len(valid_diff) // 20, 1)
                biaslist = valid_diff[::step].tolist()
            print(f"ã€unseenè®¡ç®—ã€‘biaslisté•¿åº¦ï¼š{len(biaslist)}")

        # ====================== biaså¾ªç¯è®¡ç®—ï¼ˆä¿®å¤åï¼‰======================
        seen_acc, unseen_acc = [stats["closed_seen_match"]], [stats["closed_unseen_match"]]
        base_scores = torch.stack([allpred[(a,o)] for a,o in self.dset.pairs], 1)
        # ç»´åº¦æ ¡éªŒ
        assert base_scores.shape == scores.shape, f"base_scoresç»´åº¦å¼‚å¸¸ {base_scores.shape}ï¼Œé¢„æœŸ{scores.shape}"

        for b in biaslist:
            # è°ƒç”¨score_fast_modelï¼Œå–seen/unseenå‡†ç¡®ç‡
            s,u = process(self.score_fast_model(base_scores.clone(), obj_gt, b, topk))[3:]
            seen_acc.append(s.mean().item() if len(s) else 0.0)
            unseen_acc.append(u.mean().item() if len(u) else 0.0)

        seen_acc, unseen_acc = np.array(seen_acc), np.array(unseen_acc)
        hm = hmean([seen_acc, unseen_acc], axis=0) if len(seen_acc) and len(unseen_acc) else 0.0
        # æœ€ç»ˆæŒ‡æ ‡è¿”å›
        return {
            **stats,
            "AUC": np.trapz(seen_acc, unseen_acc) if len(seen_acc) > 1 else 0.0,
            "best_hm": np.max(hm) if len(hm) else 0.0,
            "best_seen": np.max(seen_acc) if len(seen_acc) else 0.0,
            "best_unseen": np.max(unseen_acc) if len(unseen_acc) else 0.0,
            "biasterm": biaslist[np.argmax(hm)] if len(hm) and len(biaslist) else 1e3
        }

    def score_fast_model(self, scores, obj_truth, bias=0.0, topk=1):
        scores[~self.seen_mask.repeat(scores.shape[0],1)] += bias
        closed = scores.masked_fill(~self.closed_mask.repeat(scores.shape[0],1), -1e10)
        _, pred = closed.topk(topk,1)
        pred = pred.view(-1)
        return (self.pairs[pred][:,0].view(-1, topk), self.pairs[pred][:,1].view(-1, topk))
# def test(test_dset, evaluator, logits, attr_gt, obj_gt, pair_gt, config):
#     preds = {p: logits[:,i] for i,p in enumerate(test_dset.pairs)}
#     all_pred = torch.stack([preds[(a,o)] for a,o in test_dset.pairs], 1)
#     res = evaluator.score_model(preds, obj_gt, 1e3, 1)
#     attr_acc = (res['unbiased_closed'][0].squeeze(-1) == attr_gt).float().mean().item()
#     obj_acc = (res['unbiased_closed'][1].squeeze(-1) == obj_gt).float().mean().item()
#     stats = evaluator.evaluate_predictions(res, attr_gt, obj_gt, pair_gt, preds, 1)
#     return {**stats, "attr_acc": attr_acc, "obj_acc": obj_acc}
def test(test_dset, evaluator, logits, attr_gt, obj_gt, pair_gt, config):

    preds = {p: logits[:,i] for i,p in enumerate(test_dset.pairs)}
    # ğŸ”´ æ–°å¢ï¼šæ‰“å°predså€¼çš„ç»´åº¦ï¼ˆç¡®è®¤æ¯ä¸€åˆ—ç»´åº¦æ­£ç¡®ï¼‰
    pred_vals = list(preds.values())
    
    # åŸä»£ç ï¼šæ„é€ all_predå¼ é‡
    all_pred = torch.stack([preds[(a,o)] for a,o in test_dset.pairs], 1)
    
    # åŸä»£ç ï¼šè°ƒç”¨score_model
    res = evaluator.score_model(preds, obj_gt, 1e3, 1)

    
    # åŸä»£ç ï¼šè®¡ç®—å‡†ç¡®ç‡
    attr_acc = (res['unbiased_closed'][0].squeeze(-1) == attr_gt).float().mean().item()
    obj_acc = (res['unbiased_closed'][1].squeeze(-1) == obj_gt).float().mean().item()
    
    # åŸä»£ç ï¼šè¯„ä¼°é¢„æµ‹ç»“æœ
    stats = evaluator.evaluate_predictions(res, attr_gt, obj_gt, pair_gt, preds, 1)
    return {**stats, "attr_acc": attr_acc, "obj_acc": obj_acc}
# ====================== æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆé€‚é…ymlè¯»å–+è°ƒå‚é€»è¾‘ï¼‰======================
# ====================== å·¥å…·å‡½æ•°ï¼šåŠ è½½/ä¿®æ”¹é…ç½®ï¼ˆå®Œå…¨å¯¹é½æ—§è„šæœ¬ï¼Œé€‚é…åˆ†é˜¶æ®µè°ƒå‚ï¼‰======================
def load_config(cfg_path):
    """å®Œå…¨å¤ç”¨æ—§è„šæœ¬çš„é…ç½®åŠ è½½é€»è¾‘ï¼šç›´æ¥åŠ è½½ymlä¸ºå­—å…¸ï¼Œé¿å…Namespaceæ ¼å¼é—®é¢˜"""
    import yaml
    with open(cfg_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def save_config(config, save_path):
    """æ—§è„šæœ¬é…å¥—çš„é…ç½®ä¿å­˜å‡½æ•°ï¼Œä¿è¯ymlæ ¼å¼æ­£ç¡®"""
    import yaml
    with open(save_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, sort_keys=False)

def modify_config(original_cfg, tune_params, param_values):
    """
    æ ¸å¿ƒä¿®å¤ï¼šå®Œå…¨å¯¹é½æ—§è„šæœ¬çš„é…ç½®ä¿®æ”¹é€»è¾‘
    1. lambda_orth/hier_theta å¼ºåˆ¶å­˜å…¥`tta`åµŒå¥—èŠ‚ç‚¹ï¼ˆä»£ç é¢„æœŸè¯»å–ä½ç½®ï¼‰
    2. correction_interval/sim_threshold å­˜å…¥é¡¶å±‚ï¼ˆæ”¹è¿›äºŒå‚æ•°é»˜è®¤ä½ç½®ï¼‰
    3. ä¿ç•™åŸå§‹ymlæ‰€æœ‰é…ç½®ï¼Œä»…ä¿®æ”¹è°ƒå‚å‚æ•°
    4. è‡ªåŠ¨è¯†åˆ«é˜¶æ®µï¼Œé˜¶æ®µ2å›ºå®šæ”¹è¿›ä¸€æœ€ä¼˜å‚æ•°
    """
    # æ·±æ‹·è´åŸå§‹é…ç½®ï¼Œé¿å…ä¿®æ”¹åŸæ–‡ä»¶
    cfg = copy.deepcopy(original_cfg)
    # ç¡®ä¿æ ¸å¿ƒåµŒå¥—èŠ‚ç‚¹å­˜åœ¨ï¼ˆæ—§è„šæœ¬é€»è¾‘ï¼Œé¿å…é”®ä¸å­˜åœ¨æŠ¥é”™ï¼‰
    if "tta" not in cfg:
        cfg["tta"] = {}
    if "test" not in cfg:
        cfg["test"] = {}
    
    # 1. ä»ymlè¯»å–é˜¶æ®µå¼€å…³ï¼Œåˆ¤æ–­å½“å‰è°ƒå‚é˜¶æ®µ
    use_robust_cache = cfg.get("use_robust_cache", False)
    param1, param2 = tune_params["param_names"]
    val1, val2 = param_values
    
    # 2. é˜¶æ®µ1ï¼šä»…æ”¹è¿›ä¸€ï¼ˆuse_robust_cache=Falseï¼‰â†’ æ”¹ttaèŠ‚ç‚¹ä¸‹çš„lambda_orth/hier_theta
    if not use_robust_cache:
        cfg["tta"][param1] = val1
        cfg["tta"][param2] = val2
        # å¼ºåˆ¶å…³é—­æ”¹è¿›äºŒï¼Œå¯¹é½é˜¶æ®µ1éœ€æ±‚ï¼ˆæ—§è„šæœ¬FIXED_PARAMSé€»è¾‘ï¼‰
        cfg["tta"]["use_img_cache"] = False
    # 3. é˜¶æ®µ2ï¼šæ”¹è¿›ä¸€+äºŒï¼ˆuse_robust_cache=Trueï¼‰â†’ æ”¹é¡¶å±‚çš„correction_interval/sim_thresholdï¼Œå›ºå®šæ”¹è¿›ä¸€
    else:
        # åŠ è½½é˜¶æ®µ1æœ€ä¼˜å‚æ•°ï¼Œå›ºå®šåˆ°ttaèŠ‚ç‚¹ï¼ˆæ ¸å¿ƒï¼šå’Œæ—§è„šæœ¬ä¸€è‡´ï¼Œå­˜åœ¨ttaä¸‹ï¼‰
        improve1_best = load_improve1_best_params(tune_params["improve1_best_params_path"])
        cfg["tta"]["lambda_orth"] = improve1_best["best_lambda_orth"]
        cfg["tta"]["hier_theta"] = improve1_best["best_hier_theta"]
        # å¼€å¯æ”¹è¿›äºŒï¼Œå¯¹é½é˜¶æ®µ2éœ€æ±‚
        cfg["tta"]["use_img_cache"] = True
        # ä¿®æ”¹æ”¹è¿›äºŒçš„è°ƒå‚å‚æ•°ï¼ˆé¡¶å±‚ï¼Œå’Œymlé…ç½®ä¸€è‡´ï¼‰
        cfg[param1] = val1
        cfg[param2] = val2
    
    # 4. å›ºåŒ–åŸºç¡€å‚æ•°ï¼ˆå¯¹é½æ—§è„šæœ¬FIXED_PARAMSï¼Œåˆ†é…åˆ°å¯¹åº”èŠ‚ç‚¹ï¼‰
    fixed_params = {
        "open_world": False, "text_first": True, "use_wandb": True, "seed": 42,
        "eval_batch_size_wo_tta": 1, "num_workers": 0, "threshold_trials": 6,
        "shot_capacity": 3, "use_tta": True
    }
    test_params = ["open_world", "text_first", "use_wandb", "seed", "eval_batch_size_wo_tta", "num_workers", "threshold_trials"]
    tta_params = ["shot_capacity", "use_tta", "use_img_cache"]
    for k, v in fixed_params.items():
        if k in test_params:
            cfg["test"][k] = v
        elif k in tta_params and k in cfg["tta"]:
            cfg["tta"][k] = v
    
    # 5. ç”Ÿæˆä¸´æ—¶é…ç½®æ–‡ä»¶åï¼ˆæ ¼å¼åŒ–ï¼Œé¿å…æµ®ç‚¹æ•°/ç‰¹æ®Šå­—ç¬¦é—®é¢˜ï¼‰
    val1_fmt = f"{val1:.4f}" if isinstance(val1, float) else str(val1)
    val2_fmt = f"{val2:.4f}" if isinstance(val2, float) else str(val2)
    temp_cfg_path = f"temp_tune_{param1}_{val1_fmt}_{param2}_{val2_fmt}.yml"
    # ä¿å­˜ä¸´æ—¶é…ç½®ï¼ˆå®Œå…¨å¯¹é½æ—§è„šæœ¬æ ¼å¼ï¼‰
    save_config(cfg, temp_cfg_path)
    return temp_cfg_path


# def load_improve1_best_params(params_path):
#     """åŠ è½½é˜¶æ®µ1æœ€ä¼˜æ”¹è¿›ä¸€å‚æ•°ï¼ˆé˜¶æ®µ2ç”¨ï¼‰"""
#     if not os.path.exists(params_path):
#         raise FileNotFoundError(f"è¯·å…ˆè¿è¡Œé˜¶æ®µ1ï¼ˆuse_robust_cache=Falseï¼‰ç”Ÿæˆ{params_path}")
#     with open(params_path, 'r') as f:
#         return json.load(f)

# ====================== å®éªŒè¿è¡Œ+è®°å½•+å¯è§†åŒ–ï¼ˆé€‚é…åŠ¨æ€è°ƒå‚ï¼‰======================
# ====================== æ ¸å¿ƒå‡½æ•°ï¼šè¿è¡Œå•æ¬¡å®éªŒï¼ˆ100%å¤ç”¨æ—§è„šæœ¬å¯è¡Œé€»è¾‘ï¼‰======================
def run_experiment(temp_cfg_path):
    """
    å®Œå…¨å¤ç”¨æ—§è„šæœ¬çš„å®éªŒè¿è¡Œé€»è¾‘ï¼Œæ— ä»»ä½•ä¿®æ”¹ï¼
    é…ç½®è§£æâ†’æ•°æ®é›†åŠ è½½â†’æ¨¡å‹å®ä¾‹åŒ–â†’é¢„æµ‹â†’æŒ‡æ ‡è®¡ç®—ï¼Œå’Œæ—§è„šæœ¬å®Œå…¨ä¸€è‡´
    """
    sys.path.append(DIR_PATH)
    from parameters import parser
    from utils import load_args, set_seed
    from dataset import CompositionDataset
    from model.model_factory import get_model
    from swan_test_hitomcat import predict_logits_text_first_with_hitomcat  # ä½ çš„æ”¹è¿›ä¸€å‡½æ•°

    try:
        # 1. é…ç½®è§£æï¼ˆå’Œæ—§è„šæœ¬/ swan_test2.pyå®Œå…¨ä¸€è‡´ï¼‰
        args = parser.parse_args(["--cfg", temp_cfg_path])
        load_args(args.cfg, args)
        config = args
        set_seed(config.seed)
        config.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"  ğŸ“Œ ä½¿ç”¨è®¾å¤‡ï¼š{config.device}")

        # 2. å®ä¾‹åŒ–æµ‹è¯•æ•°æ®é›†ï¼ˆå®Œå…¨å¯¹é½æ—§è„šæœ¬ï¼‰
        print(f"  ğŸ“Œ åŠ è½½æ•°æ®é›†ï¼š{config.dataset}")
        test_dataset = CompositionDataset(
            config.dataset_path,
            phase='test',
            split='compositional-split-natural',
            open_world=config.open_world
        )

        # 3. æ¨¡å‹åŠ è½½ï¼ˆå®Œå…¨å¯¹é½æ—§è„šæœ¬ï¼Œå‚æ•°å¤„ç†ä¸€è‡´ï¼‰
        print(f"  ğŸ“Œ åŠ è½½æ¨¡å‹ï¼š{config.load_model}")
        allattrs = test_dataset.attrs
        allobj = test_dataset.objs
        classes = [cla.replace(".", " ").lower() for cla in allobj]
        attributes = [attr.replace(".", " ").lower() for attr in allattrs]
        offset = len(attributes)
        model = get_model(config, attributes=attributes, classes=classes, offset=offset).to(config.device)
        if config.load_model and os.path.exists(config.load_model):
            model.load_state_dict(torch.load(config.load_model, map_location='cpu'))
        model.eval()

        # 4. é€‰æ‹©é¢„æµ‹å‡½æ•°ï¼ˆå’Œæ—§è„šæœ¬ä¸€è‡´ï¼Œå¯ç”¨æ”¹è¿›ä¸€ï¼‰
        predict_logits_func = predict_logits_text_first_with_hitomcat
        print(f"  ğŸ“Œ ä½¿ç”¨é¢„æµ‹å‡½æ•°ï¼šHi-TOMCATï¼ˆæ”¹è¿›ä¸€ï¼‰")

        # 5. è¿è¡Œé¢„æµ‹ï¼ˆå¸¦autocastï¼Œå¯¹é½æ—§è„šæœ¬ï¼‰
        print(f"  ğŸ“Œ å¼€å§‹é¢„æµ‹...")
        with autocast(dtype=torch.bfloat16):
            all_logits, all_attr_gt, all_obj_gt, all_pair_gt = predict_logits_func(model, test_dataset, config)

        # 6. è®¡ç®—æŒ‡æ ‡ï¼ˆå¤ç”¨æ—§è„šæœ¬çš„Evaluator+testå‡½æ•°ï¼Œä¿è¯æŒ‡æ ‡ä¸€è‡´ï¼‰
        print(f"  ğŸ“Œ è®¡ç®—æŒ‡æ ‡...")
        evaluator = Evaluator(test_dataset, model=None, device=config.device)
        test_stats = test(test_dataset, evaluator, all_logits, all_attr_gt, all_obj_gt, all_pair_gt, config)

        # æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶
        if os.path.exists(temp_cfg_path):
            os.remove(temp_cfg_path)

        # æå–æ ¸å¿ƒæŒ‡æ ‡
        res = {k: test_stats.get(k, 0.0) for k in CORE_METRICS}
        print(f"  âœ… å®éªŒå®Œæˆ | AUC: {res['AUC']:.4f} | Best HM: {res['best_hm']:.4f}")
        return res

    except Exception as e:
        # å¤±è´¥æ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_cfg_path):
            os.remove(temp_cfg_path)
        raise Exception(f"è¿è¡Œå¼‚å¸¸ï¼š{str(e)}")

def load_improve1_best_params(params_path):
    """åŠ è½½é˜¶æ®µ1æœ€ä¼˜å‚æ•°ï¼Œé€‚é…é˜¶æ®µ2å›ºå®šéœ€æ±‚"""
    if not os.path.exists(params_path):
        raise FileNotFoundError(
            f"è¯·å…ˆè¿è¡Œé˜¶æ®µ1ï¼ˆymlä¸­use_robust_cache=Falseï¼‰ç”Ÿæˆæœ€ä¼˜å‚æ•°æ–‡ä»¶ï¼\nç¼ºå¤±æ–‡ä»¶ï¼š{params_path}"
        )
    with open(params_path, 'r', encoding='utf-8') as f:
        best_params = json.load(f)
    # å…¼å®¹æ—§è„šæœ¬çš„å‚æ•°åï¼Œç¡®ä¿èƒ½æ­£ç¡®è¯»å–
    if "best_lambda_orth" not in best_params or "best_hier_theta" not in best_params:
        raise KeyError("é˜¶æ®µ1æœ€ä¼˜å‚æ•°æ–‡ä»¶ç¼ºå°‘æ ¸å¿ƒé”®ï¼šbest_lambda_orth / best_hier_theta")
    return best_params
def init_record(save_dir, param_names):
    """åˆå§‹åŒ–CSVå’ŒSwanLab"""
    os.makedirs(save_dir, exist_ok=True)
    csv_path = os.path.join(save_dir, "tune_metrics.csv")
    headers = param_names + CORE_METRICS
    with open(csv_path, 'w', newline='') as f:
        csv.DictWriter(f, fieldnames=headers).writeheader()
    # SwanLabé¡¹ç›®å=ä¿å­˜ç›®å½•å
    swanlab.init(project=os.path.basename(save_dir), config={"tune_params": param_names})
    return csv_path

def record_data(csv_path, param_names, param_values, metrics):
    """è®°å½•æ•°æ®åˆ°CSV+SwanLab"""
    row = dict(zip(param_names, param_values))
    row.update(metrics)
    with open(csv_path, 'a+', newline='') as f:
        csv.DictWriter(f, fieldnames=row.keys()).writerow(row)
    swanlab.log({**metrics, **dict(zip(param_names, param_values))})

def visualize(save_dir, csv_path, param_names):
    """å¯è§†åŒ–è°ƒå‚ç»“æœ"""
    import pandas as pd
    df = pd.read_csv(csv_path).dropna(subset=["AUC"]).query("AUC>0")
    if len(df) == 0:
        print("æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–")
        return

    # çƒ­åŠ›å›¾ï¼ˆAUCï¼‰
    for value in ["AUC", "best_hm", "attr_acc"]:
        plt.figure(figsize=(10,8))
        pivot = df.pivot(index=param_names[0], columns=param_names[1], values=value)
        im = plt.imshow(pivot, cmap="YlGnBu", aspect="auto")
        for i in range(len(pivot.index)):
            for j in range(len(pivot.columns)):
                plt.text(j, i, f"{pivot.iloc[i,j]:.4f}", ha="center", va="center", fontsize=10)
        plt.colorbar(im, label=value)
        plt.xlabel(param_names[1], fontsize=14, fontweight="bold")
        plt.ylabel(param_names[0], fontsize=14, fontweight="bold")
        plt.title(f"{value} Heatmap (Higher is Better)", fontsize=16, fontweight="bold")
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, f"{value}_heatmap.png"), dpi=300)
        plt.close()

    # æŠ˜çº¿å›¾ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰
    for metric in ["AUC", "best_hm", "attr_acc"]:
        plt.figure(figsize=(12,6))
        for p2 in df[param_names[1]].unique():
            data = df[df[param_names[1]] == p2].sort_values(param_names[0])
            plt.plot(data[param_names[0]], data[metric], marker="o", linewidth=2, label=f"{param_names[1]}={p2}")
        plt.xlabel(param_names[0], fontsize=14, fontweight="bold")
        plt.ylabel(metric, fontsize=14, fontweight="bold")
        plt.title(f"{metric} vs {param_names[0]}", fontsize=16, fontweight="bold")
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, f"{metric}_lineplot.png"), dpi=300)
        plt.close()

    # ä¿å­˜æœ€ä¼˜å‚æ•°
    best_row = df.loc[df["AUC"].idxmax()]
    best_params = {**dict(zip(param_names, best_row[param_names])), **best_row[CORE_METRICS].to_dict()}
    with open(os.path.join(save_dir, "best_params.json"), 'w') as f:
        json.dump(best_params, f, indent=4)

    # æ‰“å°ç»“æœ
    print("="*80)
    print(f"âœ… è°ƒå‚å®Œæˆï¼æœ€ä¼˜å‚æ•°ï¼š")
    for k,v in best_params.items():
        if k in param_names:
            print(f"ğŸ“Œ {k}: {v:.4f}" if isinstance(v, float) else f"ğŸ“Œ {k}: {v}")
        elif k in ["AUC", "best_hm"]:
            print(f"ğŸ“Š {k}: {v:.4f}")
        else:
            print(f"ğŸ“Š {k}: {v:.2%}")
    print("="*80)

# ====================== ä¸»å‡½æ•°ï¼ˆè‡ªåŠ¨è¯†åˆ«é˜¶æ®µ+ç½‘æ ¼æœç´¢ï¼‰======================

# ====================== ä¸»å‡½æ•°ï¼ˆè‡ªåŠ¨è¯†åˆ«é˜¶æ®µ+ç½‘æ ¼æœç´¢ï¼Œä¿®å¤å­—å…¸è®¿é—®é”™è¯¯ï¼‰======================
def main():
    # 1. åŠ è½½ymlé…ç½®ï¼ˆå­—å…¸ï¼‰ï¼Œä»ttaèŠ‚ç‚¹è¯»å–é˜¶æ®µå¼€å…³ï¼Œä¿®å¤å­—å…¸è®¿é—®é”™è¯¯
    original_cfg = load_config(CFG_PATH)
    # æ ¸å¿ƒä¿®å¤ï¼šä»ttaåµŒå¥—èŠ‚ç‚¹è¯»å–use_robust_cacheï¼Œå­—å…¸ç”¨[]è®¿é—®ï¼ŒåŠ é»˜è®¤å€¼é¿å…é”®ä¸å­˜åœ¨
    use_robust_cache = original_cfg.get("tta", {}).get("use_robust_cache", False)
    
    if not use_robust_cache:
        # STAGE1ï¼šä»…æ”¹è¿›ä¸€ï¼Œè°ƒlambda_orth+hier_theta
        tune_params = TUNE_PARAMS_STAGE1
        save_dir = f"{SAVE_DIR_PREFIX}stage1_improve1_only"
    else:
        # STAGE2ï¼šæ”¹è¿›ä¸€+äºŒï¼Œè°ƒcorrection_interval+sim_thresholdï¼ˆå›ºå®šæ”¹è¿›ä¸€ï¼‰
        tune_params = TUNE_PARAMS_STAGE2
        save_dir = f"{SAVE_DIR_PREFIX}stage2_improve1fixed_improve2"
        # æå‰åŠ è½½é˜¶æ®µ1æœ€ä¼˜å‚æ•°ï¼ˆä»…æ‰“å°ç”¨ï¼Œmodify_configä¸­ä¼šå®é™…è®¾ç½®åˆ°é…ç½®ï¼‰
        improve1_best = load_improve1_best_params(tune_params["improve1_best_params_path"])
        print(f"ğŸ“Œ å›ºå®šæ”¹è¿›ä¸€æœ€ä¼˜å‚æ•°ï¼šlambda_orth={improve1_best['best_lambda_orth']:.4f}, hier_theta={improve1_best['best_hier_theta']:.4f}")

    # 2. åˆå§‹åŒ–è®°å½•
    csv_path = init_record(save_dir, tune_params["param_names"])
    print("="*80)
    print(f"ğŸš€ å¼€å§‹è°ƒå‚ï¼ˆé˜¶æ®µï¼š{'ä»…æ”¹è¿›ä¸€' if not use_robust_cache else 'æ”¹è¿›ä¸€+äºŒ'}ï¼‰")
    print(f"ğŸ“Œ è°ƒå‚å‚æ•°ï¼š{tune_params['param_names']}")
    print(f"ğŸ“Œ å‚æ•°èŒƒå›´ï¼š{tune_params['ranges']}")
    print(f"ğŸ“Œ æ€»å®éªŒç»„æ•°ï¼š{len(tune_params['ranges'][0]) * len(tune_params['ranges'][1])}")
    print(f"ğŸ“Œ ç»“æœä¿å­˜è‡³ï¼š{save_dir}")
    print("="*80)

    # 3. ç½‘æ ¼æœç´¢
    param_combinations = product(*tune_params["ranges"])
    total = len(tune_params['ranges'][0]) * len(tune_params['ranges'][1])
    success = 0

    for idx, param_vals in enumerate(param_combinations, 1):
        print(f"\n{'='*60}")
        print(f"ã€å®éªŒ {idx}/{total}ã€‘{dict(zip(tune_params['param_names'], param_vals))}")
        print(f"{'='*60}")
        try:
            # ç”Ÿæˆä¸´æ—¶é…ç½®ï¼ˆmodify_configä¸­å·²å¤„ç†æ‰€æœ‰å‚æ•°è®¾ç½®ï¼ŒåŒ…æ‹¬é˜¶æ®µ2å›ºå®šæ”¹è¿›ä¸€ï¼‰
            temp_cfg = modify_config(original_cfg, tune_params, param_vals)
            # è¿è¡Œå®éªŒ
            metrics = run_experiment(temp_cfg)
            # è®°å½•æ•°æ®
            record_data(csv_path, tune_params["param_names"], param_vals, metrics)
            success += 1
            print(f"âœ… æˆåŠŸ | AUC: {metrics['AUC']:.4f} | Best HM: {metrics['best_hm']:.4f}")
        except Exception as e:
            print(f"âŒ å¤±è´¥ | é”™è¯¯ï¼š{str(e)}...")
            continue

    # 4. å¯è§†åŒ–+æ€»ç»“
    visualize(save_dir, csv_path, tune_params["param_names"])
    swanlab.finish()
    print(f"\nğŸ“Š å®éªŒæ€»ç»“ï¼šå…±{total}ç»„ | æˆåŠŸ{success}ç»„ | å¤±è´¥{total-success}ç»„")
    if success == 0:
        print("âŒ æ‰€æœ‰å®éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥ï¼š1.ymlè·¯å¾„/å‚æ•°æ˜¯å¦æ­£ç¡® 2.æ¨¡å‹/æ•°æ®é›†è·¯å¾„æ˜¯å¦æœ‰æ•ˆ 3.é˜¶æ®µ1æœ€ä¼˜å‚æ•°æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆé˜¶æ®µ2ï¼‰")
if __name__ == "__main__":
    main()