import os
import sys
import csv
import json
import copy
import random
import torch
import numpy as np
import matplotlib.pyplot as plt
import swanlab
from itertools import product
from datetime import datetime
from torch.cuda.amp import autocast

# ====================== å…¨å±€é…ç½®ï¼šä»…æŒ‡å®šã€å½“å‰é˜¶æ®µè¦è°ƒçš„2ä¸ªå‚æ•°èŒƒå›´ã€‘+ åŸºç¡€é€‚é… ======================
plt.switch_backend('Agg')
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ---------------------- ä»…éœ€ä¿®æ”¹è¿™é‡Œï¼æŒ‡å®šå½“å‰è¦è°ƒçš„2ä¸ªå‚æ•°èŒƒå›´ ----------------------
# æ³¨æ„ï¼šè°ƒå‚èŒƒå›´äºŒé€‰ä¸€ï¼Œæ ¹æ®ymlä¸­use_robust_cacheå¼€å…³åŒ¹é…
# 1. STAGE1ï¼ˆymlä¸­use_robust_cache=Falseï¼‰ï¼šä»…æ”¹è¿›ä¸€ï¼Œè°ƒä»¥ä¸‹2ä¸ªå‚æ•°
TUNE_PARAMS_STAGE1 = {
    "param_names": ["lambda_orth", "hier_theta"],
    "ranges": [
        [0.0001, 0.0005, 0.0008, 0.001, 0.002, 0.003, 0.005, 0.006, 0.007],  # lambda_orthèŒƒå›´
        [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]                        # hier_thetaèŒƒå›´
    ]
}

# 2. STAGE2ï¼ˆymlä¸­use_robust_cache=Trueï¼‰ï¼šæ”¹è¿›ä¸€+äºŒï¼Œè°ƒä»¥ä¸‹2ä¸ªå‚æ•°ï¼ˆæ”¹è¿›ä¸€å›ºå®šï¼‰
TUNE_PARAMS_STAGE2 = {
    "param_names": ["correction_interval", "sim_threshold"],
    "ranges": [
        [10, 20, 30, 40],                          # correction_intervalèŒƒå›´
        [0.10, 0.15, 0.20, 0.25, 0.30]             # sim_thresholdèŒƒå›´
    ],
    "improve1_best_params_path": "tune_improve1_only_results/best_params.json"  # é˜¶æ®µ1æœ€ä¼˜å‚æ•°è·¯å¾„
}

# ---------------------- å›ºå®šé…ç½®ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰ ----------------------
CFG_PATH = "config/ut-zappos.yml"  # ä½ çš„ymlè·¯å¾„
CORE_METRICS = ["AUC", "best_hm", "attr_acc", "best_seen", "best_unseen", "obj_acc", "biasterm"]
SAVE_DIR_PREFIX = "tune_results_"  # ç»“æœä¿å­˜ç›®å½•å‰ç¼€ï¼ˆè‡ªåŠ¨åŠ é˜¶æ®µåï¼‰

# ====================== å¯¹é½swan_test2.pyçš„å·¥å…·ç±»ï¼ˆæ— ä¿®æ”¹ï¼‰======================
DIR_PATH = os.path.dirname(os.path.abspath(__file__))
class Evaluator:
    def __init__(self, dset, model, device):
        self.dset = dset
        self.device = device
        pairs = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in dset.pairs]
        self.train_pairs = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in dset.train_pairs]
        self.pairs = torch.LongTensor(pairs)

        if dset.phase == 'train':
            test_pair_set = set(dset.train_pairs)
            test_pair_gt = set(dset.train_pairs)
        elif dset.phase == 'val':
            test_pair_set = set(dset.val_pairs + dset.train_pairs)
            test_pair_gt = set(dset.val_pairs)
        else:
            test_pair_set = set(dset.test_pairs + dset.train_pairs)
            test_pair_gt = set(dset.test_pairs)

        self.test_pair_dict = [(dset.attr2idx[attr], dset.obj2idx[obj]) for attr, obj in test_pair_gt]
        self.test_pair_dict = dict.fromkeys(self.test_pair_dict, 0)
        for attr, obj in test_pair_gt:
            pair_val = dset.pair2idx[(attr, obj)]
            key = (dset.attr2idx[attr], dset.obj2idx[obj])
            self.test_pair_dict[key] = [pair_val, 0, 0]

        masks = [1 for _ in dset.pairs] if dset.open_world else [1 if pair in test_pair_set else 0 for pair in dset.pairs]
        self.closed_mask = torch.BoolTensor(masks)
        seen_mask = [1 if pair in set(dset.train_pairs) else 0 for pair in dset.pairs]
        self.seen_mask = torch.BoolTensor(seen_mask)

        oracle_obj_mask = []
        for _obj in dset.objs:
            oracle_obj_mask.append(torch.BoolTensor([1 if _obj == obj else 0 for attr, obj in dset.pairs]))
        self.oracle_obj_mask = torch.stack(oracle_obj_mask, 0)
        self.score_model = self.score_manifold_model

    def generate_predictions(self, scores, obj_truth, bias=0.0, topk=1):
        def get_pred(_s):
            _, pred = _s.topk(topk, dim=1)
            pred = pred.view(-1)
            return self.pairs[pred][:,0].view(-1,topk), self.pairs[pred][:,1].view(-1,topk)
        orig = scores.clone()
        scores[~self.seen_mask.repeat(scores.shape[0],1)] += bias
        return {
            "open": get_pred(scores),
            "unbiased_open": get_pred(orig),
            "closed": get_pred(scores.masked_fill(~self.closed_mask.repeat(scores.shape[0],1), -1e10)),
            "unbiased_closed": get_pred(orig.masked_fill(~self.closed_mask.repeat(scores.shape[0],1), -1e10))
        }

    def score_manifold_model(self, scores, obj_truth, bias=0.0, topk=1):
        scores = torch.stack([scores[(a,o)] for a,o in self.dset.pairs], 1)
        return {**self.generate_predictions(scores, obj_truth, bias, topk), "scores": scores.clone()}

    def evaluate_predictions(self, preds, attr_gt, obj_gt, pair_gt, allpred, topk=1):
        from scipy.stats import hmean
        attr_gt, obj_gt, pair_gt = attr_gt.cpu(), obj_gt.cpu(), pair_gt.cpu()
        seen_ind = torch.tensor([i for i,(a,o) in enumerate(zip(attr_gt.numpy(), obj_gt.numpy())) if (a,o) in self.train_pairs])
        unseen_ind = torch.tensor([i for i,(a,o) in enumerate(zip(attr_gt.numpy(), obj_gt.numpy())) if (a,o) not in self.train_pairs])

        def process(s):
            a_match = (attr_gt.unsqueeze(1).repeat(1,topk) == s[0][:,:topk]).any(1).float()
            o_match = (obj_gt.unsqueeze(1).repeat(1,topk) == s[1][:,:topk]).any(1).float()
            match = (a_match * o_match).float()
            return a_match, o_match, match, match[seen_ind], match[unseen_ind], torch.ones(512,5), torch.ones(512,5), torch.ones(512,5)

        stats = {}
        for k in ["closed", "unbiased_closed"]:
            a,o,m,s,u,sc,ss,su = process(preds[k])
            stats[f"{k}_attr_match"] = a.mean().item()
            stats[f"{k}_obj_match"] = o.mean().item()
            stats[f"{k}_match"] = m.mean().item()
            stats[f"{k}_seen_match"] = s.mean().item() if len(s) else 0.0
            stats[f"{k}_unseen_match"] = u.mean().item() if len(u) else 0.0

        scores = preds["scores"]
        correct_scores = scores[torch.arange(len(scores)), pair_gt][unseen_ind]
        max_seen = scores[unseen_ind][:, self.seen_mask].topk(topk,1)[0][:,topk-1]
        diff = max_seen - correct_scores
        valid_diff = diff[stats["closed_unseen_match"]>0] - 1e-4
        biaslist = valid_diff[::max(len(valid_diff)//20,1)] if len(valid_diff) else [0.0]

        seen_acc, unseen_acc = [stats["closed_seen_match"]], [stats["closed_unseen_match"]]
        base_scores = torch.stack([allpred[(a,o)] for a,o in self.dset.pairs], 1)
        for b in biaslist:
            s,u = process(self.score_fast_model(base_scores.clone(), obj_gt, b, topk))[3:5]
            seen_acc.append(s.mean().item() if len(s) else 0.0)
            unseen_acc.append(u.mean().item() if len(u) else 0.0)

        seen_acc, unseen_acc = np.array(seen_acc), np.array(unseen_acc)
        hm = hmean([seen_acc, unseen_acc], axis=0) if len(seen_acc) else 0.0
        return {
            **stats,
            "AUC": np.trapz(seen_acc, unseen_acc),
            "best_hm": np.max(hm) if len(hm) else 0.0,
            "best_seen": np.max(seen_acc),
            "best_unseen": np.max(unseen_acc),
            "biasterm": biaslist[np.argmax(hm)] if len(hm) else 1e3
        }

    def score_fast_model(self, scores, obj_truth, bias=0.0, topk=1):
        scores[~self.seen_mask.repeat(scores.shape[0],1)] += bias
        closed = scores.masked_fill(~self.closed_mask.repeat(scores.shape[0],1), -1e10)
        _, pred = closed.topk(topk,1)
        pred = pred.view(-1)
        return (self.pairs[pred][:,0].view(-1,topk), self.pairs[pred][:,1].view(-1,topk))

def test(test_dset, evaluator, logits, attr_gt, obj_gt, pair_gt, config):
    preds = {p: logits[:,i] for i,p in enumerate(test_dset.pairs)}
    all_pred = torch.stack([preds[(a,o)] for a,o in test_dset.pairs], 1)
    res = evaluator.score_model(preds, obj_gt, 1e3, 1)
    attr_acc = (res['unbiased_closed'][0].squeeze(-1) == attr_gt).float().mean().item()
    obj_acc = (res['unbiased_closed'][1].squeeze(-1) == obj_gt).float().mean().item()
    stats = evaluator.evaluate_predictions(res, attr_gt, obj_gt, pair_gt, preds, 1)
    return {**stats, "attr_acc": attr_acc, "obj_acc": obj_acc}

# ====================== æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆé€‚é…ymlè¯»å–+è°ƒå‚é€»è¾‘ï¼‰======================
def load_config(cfg_path):
    """åŠ è½½ymlé…ç½®ï¼Œè¿”å›Namespaceå¯¹è±¡ï¼ˆå’Œswan_test2.pyä¸€è‡´ï¼‰"""
    import yaml
    from parameters import parser
    args = parser.parse_args(["--cfg", cfg_path])
    from utils import load_args
    load_args(args.cfg, args)
    return args

def load_improve1_best_params(params_path):
    """åŠ è½½é˜¶æ®µ1æœ€ä¼˜æ”¹è¿›ä¸€å‚æ•°ï¼ˆé˜¶æ®µ2ç”¨ï¼‰"""
    if not os.path.exists(params_path):
        raise FileNotFoundError(f"è¯·å…ˆè¿è¡Œé˜¶æ®µ1ï¼ˆuse_robust_cache=Falseï¼‰ç”Ÿæˆ{params_path}")
    with open(params_path, 'r') as f:
        return json.load(f)

def modify_config(original_cfg, tune_params, param_values):
    """
    åŠ¨æ€ä¿®æ”¹é…ç½®ï¼šä»…è¦†ç›–å½“å‰è¦è°ƒçš„2ä¸ªå‚æ•°ï¼Œå…¶ä½™ä¿ç•™ymlå€¼
    original_cfg: ä»ymlåŠ è½½çš„åŸå§‹é…ç½®
    tune_params: å½“å‰é˜¶æ®µçš„è°ƒå‚é…ç½®ï¼ˆparam_names + rangesï¼‰
    param_values: æœ¬æ¬¡å®éªŒçš„2ä¸ªå‚æ•°å€¼
    """
    cfg = copy.deepcopy(original_cfg)
    # ä»…è¦†ç›–è¦è°ƒçš„2ä¸ªå‚æ•°
    for param_name, param_val in zip(tune_params["param_names"], param_values):
        setattr(cfg, param_name, param_val)
    # ç”Ÿæˆä¸´æ—¶é…ç½®æ–‡ä»¶ï¼ˆåŸºäºåŸå§‹ymlä¿®æ”¹ï¼Œä»…æ”¹2ä¸ªå‚æ•°ï¼‰
    temp_cfg_path = f"temp_tune_{'_'.join([str(v) for v in param_values])}.yml"
    import yaml
    with open(temp_cfg_path, 'w') as f:
        yaml.dump(vars(cfg), f, sort_keys=False)
    return temp_cfg_path

# ====================== å®éªŒè¿è¡Œ+è®°å½•+å¯è§†åŒ–ï¼ˆé€‚é…åŠ¨æ€è°ƒå‚ï¼‰======================
def run_experiment(temp_cfg_path):
    """è¿è¡Œå•æ¬¡å®éªŒï¼ˆå¤ç”¨swan_test2.pyé€»è¾‘ï¼‰"""
    sys.path.append(DIR_PATH)
    from dataset import CompositionDataset
    from model.model_factory import get_model
    from swan_test_hitomcat import predict_logits_text_first_with_hitomcat

    try:
        # åŠ è½½ä¸´æ—¶é…ç½®ï¼ˆä»…æ”¹äº†2ä¸ªè°ƒå‚å‚æ•°ï¼‰
        config = load_config(temp_cfg_path)
        config.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # åŠ è½½æ•°æ®é›†+æ¨¡å‹ï¼ˆå…¨éƒ¨ä»ymlè¯»å‚æ•°ï¼‰
        test_dset = CompositionDataset(
            config.dataset_path, phase='test', split='compositional-split-natural', open_world=config.open_world
        )
        allattrs = [a.replace("."," ").lower() for a in test_dset.attrs]
        allobj = [o.replace("."," ").lower() for o in test_dset.objs]
        model = get_model(config, attributes=allattrs, classes=allobj, offset=len(allattrs)).to(config.device)
        if config.load_model and os.path.exists(config.load_model):
            model.load_state_dict(torch.load(config.load_model, map_location='cpu'))
        model.eval()

        # é¢„æµ‹+è®¡ç®—æŒ‡æ ‡
        with autocast(dtype=torch.bfloat16):
            logits, attr_gt, obj_gt, pair_gt = predict_logits_text_first_with_hitomcat(model, test_dset, config)
        evaluator = Evaluator(test_dset, model, config.device)
        stats = test(test_dset, evaluator, logits, attr_gt, obj_gt, pair_gt, config)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_cfg_path):
            os.remove(temp_cfg_path)
        return {k: stats.get(k, 0.0) for k in CORE_METRICS}
    except Exception as e:
        if os.path.exists(temp_cfg_path):
            os.remove(temp_cfg_path)
        raise Exception(f"å®éªŒå¤±è´¥ï¼š{str(e)}")

def init_record(save_dir, param_names):
    """åˆå§‹åŒ–CSVå’ŒSwanLab"""
    os.makedirs(save_dir, exist_ok=True)
    csv_path = os.path.join(save_dir, "tune_metrics.csv")
    headers = param_names + CORE_METRICS
    with open(csv_path, 'w', newline='') as f:
        csv.DictWriter(f, fieldnames=headers).writeheader()
    # SwanLabé¡¹ç›®å=ä¿å­˜ç›®å½•å
    swanlab.init(project=os.path.basename(save_dir), config={"tune_params": param_names})
    return csv_path

def record_data(csv_path, param_names, param_values, metrics):
    """è®°å½•æ•°æ®åˆ°CSV+SwanLab"""
    row = dict(zip(param_names, param_values))
    row.update(metrics)
    with open(csv_path, 'a+', newline='') as f:
        csv.DictWriter(f, fieldnames=row.keys()).writerow(row)
    swanlab.log({**metrics, **dict(zip(param_names, param_values))})

def visualize(save_dir, csv_path, param_names):
    """å¯è§†åŒ–è°ƒå‚ç»“æœ"""
    import pandas as pd
    df = pd.read_csv(csv_path).dropna(subset=["AUC"]).query("AUC>0")
    if len(df) == 0:
        print("æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–")
        return

    # çƒ­åŠ›å›¾ï¼ˆAUCï¼‰
    for value in ["AUC", "best_hm", "attr_acc"]:
        plt.figure(figsize=(10,8))
        pivot = df.pivot(index=param_names[0], columns=param_names[1], values=value)
        im = plt.imshow(pivot, cmap="YlGnBu", aspect="auto")
        for i in range(len(pivot.index)):
            for j in range(len(pivot.columns)):
                plt.text(j, i, f"{pivot.iloc[i,j]:.4f}", ha="center", va="center", fontsize=10)
        plt.colorbar(im, label=value)
        plt.xlabel(param_names[1], fontsize=14, fontweight="bold")
        plt.ylabel(param_names[0], fontsize=14, fontweight="bold")
        plt.title(f"{value} Heatmap (Higher is Better)", fontsize=16, fontweight="bold")
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, f"{value}_heatmap.png"), dpi=300)
        plt.close()

    # æŠ˜çº¿å›¾ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰
    for metric in ["AUC", "best_hm", "attr_acc"]:
        plt.figure(figsize=(12,6))
        for p2 in df[param_names[1]].unique():
            data = df[df[param_names[1]] == p2].sort_values(param_names[0])
            plt.plot(data[param_names[0]], data[metric], marker="o", linewidth=2, label=f"{param_names[1]}={p2}")
        plt.xlabel(param_names[0], fontsize=14, fontweight="bold")
        plt.ylabel(metric, fontsize=14, fontweight="bold")
        plt.title(f"{metric} vs {param_names[0]}", fontsize=16, fontweight="bold")
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, f"{metric}_lineplot.png"), dpi=300)
        plt.close()

    # ä¿å­˜æœ€ä¼˜å‚æ•°
    best_row = df.loc[df["AUC"].idxmax()]
    best_params = {**dict(zip(param_names, best_row[param_names])), **best_row[CORE_METRICS].to_dict()}
    with open(os.path.join(save_dir, "best_params.json"), 'w') as f:
        json.dump(best_params, f, indent=4)

    # æ‰“å°ç»“æœ
    print("="*80)
    print(f"âœ… è°ƒå‚å®Œæˆï¼æœ€ä¼˜å‚æ•°ï¼š")
    for k,v in best_params.items():
        if k in param_names:
            print(f"ğŸ“Œ {k}: {v:.4f}" if isinstance(v, float) else f"ğŸ“Œ {k}: {v}")
        elif k in ["AUC", "best_hm"]:
            print(f"ğŸ“Š {k}: {v:.4f}")
        else:
            print(f"ğŸ“Š {k}: {v:.2%}")
    print("="*80)

# ====================== ä¸»å‡½æ•°ï¼ˆè‡ªåŠ¨è¯†åˆ«é˜¶æ®µ+ç½‘æ ¼æœç´¢ï¼‰======================
def main():
    # 1. åŠ è½½ymlé…ç½®ï¼Œè‡ªåŠ¨è¯†åˆ«é˜¶æ®µ
    original_cfg = load_config(CFG_PATH)
    use_robust_cache = original_cfg.use_robust_cache
    if not use_robust_cache:
        # STAGE1ï¼šä»…æ”¹è¿›ä¸€ï¼Œè°ƒlambda_orth+hier_theta
        tune_params = TUNE_PARAMS_STAGE1
        save_dir = f"{SAVE_DIR_PREFIX}stage1_improve1_only"
        swanlab_project = "Tune-Stage1-Improve1-Only"
    else:
        # STAGE2ï¼šæ”¹è¿›ä¸€+äºŒï¼Œè°ƒcorrection_interval+sim_thresholdï¼ˆå›ºå®šæ”¹è¿›ä¸€ï¼‰
        tune_params = TUNE_PARAMS_STAGE2
        save_dir = f"{SAVE_DIR_PREFIX}stage2_improve1fixed_improve2"
        swanlab_project = "Tune-Stage2-Improve1Fixed+Improve2"
        # åŠ è½½é˜¶æ®µ1æœ€ä¼˜æ”¹è¿›ä¸€å‚æ•°ï¼Œå›ºå®šåˆ°é…ç½®
        improve1_best = load_improve1_best_params(tune_params["improve1_best_params_path"])
        setattr(original_cfg, "lambda_orth", improve1_best["best_lambda_orth"])
        setattr(original_cfg, "hier_theta", improve1_best["best_hier_theta"])
        print(f"ğŸ“Œ å›ºå®šæ”¹è¿›ä¸€æœ€ä¼˜å‚æ•°ï¼šlambda_orth={improve1_best['best_lambda_orth']:.4f}, hier_theta={improve1_best['best_hier_theta']:.4f}")

    # 2. åˆå§‹åŒ–è®°å½•
    csv_path = init_record(save_dir, tune_params["param_names"])
    print("="*80)
    print(f"ğŸš€ å¼€å§‹è°ƒå‚ï¼ˆé˜¶æ®µï¼š{'ä»…æ”¹è¿›ä¸€' if not use_robust_cache else 'æ”¹è¿›ä¸€+äºŒ'}ï¼‰")
    print(f"ğŸ“Œ è°ƒå‚å‚æ•°ï¼š{tune_params['param_names']}")
    print(f"ğŸ“Œ å‚æ•°èŒƒå›´ï¼š{tune_params['ranges']}")
    print(f"ğŸ“Œ æ€»å®éªŒç»„æ•°ï¼š{len(tune_params['ranges'][0]) * len(tune_params['ranges'][1])}")
    print(f"ğŸ“Œ ç»“æœä¿å­˜è‡³ï¼š{save_dir}")
    print("="*80)

    # 3. ç½‘æ ¼æœç´¢
    param_combinations = product(*tune_params["ranges"])
    total = len(tune_params['ranges'][0]) * len(tune_params['ranges'][1])
    success = 0

    for idx, param_vals in enumerate(param_combinations, 1):
        print(f"\n{'='*60}")
        print(f"ã€å®éªŒ {idx}/{total}ã€‘{dict(zip(tune_params['param_names'], param_vals))}")
        print(f"{'='*60}")
        try:
            temp_cfg = modify_config(original_cfg, tune_params, param_vals)
            metrics = run_experiment(temp_cfg)
            record_data(csv_path, tune_params["param_names"], param_vals, metrics)
            success += 1
            print(f"âœ… æˆåŠŸ | AUC: {metrics['AUC']:.4f} | Best HM: {metrics['best_hm']:.4f}")
        except Exception as e:
            print(f"âŒ å¤±è´¥ | é”™è¯¯ï¼š{str(e)[:100]}...")
            continue

    # 4. å¯è§†åŒ–+æ€»ç»“
    visualize(save_dir, csv_path, tune_params["param_names"])
    swanlab.finish()
    print(f"\nğŸ“Š æ€»ç»“ï¼šå…±{total}ç»„ | æˆåŠŸ{success}ç»„ | å¤±è´¥{total-success}ç»„")

if __name__ == "__main__":
    main()